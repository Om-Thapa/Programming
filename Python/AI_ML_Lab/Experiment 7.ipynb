{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "n6nqbfK2Yjh4",
   "metadata": {
    "id": "n6nqbfK2Yjh4"
   },
   "source": [
    "# Experiment 7: Feature Encoding\n",
    "\n",
    "In machine learning, models require **numerical inputs**, but datasets often contain **categorical or text data**.  \n",
    "Encoding is the process of converting **categorical or textual features into numeric representations** so that ML models can process them.\n",
    "\n",
    "- **Categorical Encoding**: Converts discrete categories (e.g., color, species) into numbers.  \n",
    "- **Text Encoding**: Converts text (sentences, documents) into numeric vectors.  \n",
    "- Proper encoding is critical for model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vu8fTXrEYnGg",
   "metadata": {
    "id": "vu8fTXrEYnGg"
   },
   "source": [
    "# Categorical Encoding Techniques\n",
    "\n",
    "Common techniques for categorical data:\n",
    "\n",
    "1. **One-Hot Encoding**\n",
    "   - Creates a binary column for each category.\n",
    "   - No ordinal assumptions are made.\n",
    "   - Example: Color = {Red, Blue, Green} → [1,0,0], [0,1,0], [0,0,1]\n",
    "\n",
    "2. **Label Encoding**\n",
    "   - Assigns a unique integer to each category.\n",
    "   - Suitable for **ordinal features** (where order matters).\n",
    "   - Example: Size = {Small, Medium, Large} → 0, 1, 2\n",
    "\n",
    "3. **Binary/Ordinal Encoding**\n",
    "   - Efficient for **high-cardinality categorical features**.\n",
    "   - Reduces the number of columns compared to One-Hot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MgvftBTzYt4b",
   "metadata": {
    "id": "MgvftBTzYt4b"
   },
   "source": [
    "# Text Encoding Techniques\n",
    "\n",
    "Text data requires encoding to be used in ML models:\n",
    "\n",
    "1. **Bag of Words (BoW)**\n",
    "   - Represents text as **word frequency vectors**.\n",
    "   - Ignores word order and semantics.\n",
    "\n",
    "2. **TF-IDF (Term Frequency–Inverse Document Frequency)**\n",
    "   - Weighted version of BoW.\n",
    "   - Common words get lower weight, rare words get higher weight.\n",
    "\n",
    "3. **Word2Vec**\n",
    "   - Neural network-based embeddings.\n",
    "   - Captures **semantic meaning**; similar words are close in vector space.\n",
    "   - Example: \"King - Man + Woman ≈ Queen\"\n",
    "\n",
    "4. **FastText**\n",
    "   - Extends Word2Vec using **subword information** (character n-grams).\n",
    "   - Handles rare or unknown words better.\n",
    "   - Useful for morphologically rich languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DhJ84Q0kY04a",
   "metadata": {
    "id": "DhJ84Q0kY04a"
   },
   "source": [
    "# Summary of Encoding\n",
    "\n",
    "- **Categorical Encoding** → One-Hot, Label, Binary/Ordinal for discrete features.  \n",
    "- **Text Encoding** → BoW, TF-IDF for classical ML; Word2Vec, FastText for modern NLP.  \n",
    "- Proper encoding ensures **numerical representation** of all features, enabling ML models to learn patterns effectively.\n",
    "- Scaling (from Experiment 6) is usually applied **after encoding**, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qyYP4b2fZzHV",
   "metadata": {
    "id": "qyYP4b2fZzHV"
   },
   "source": [
    "# Step 1: Loading Text Dataset\n",
    "\n",
    "In this step, we will load the text data file `Text_Data.json`.  \n",
    "\n",
    "- Each entry in the file represents a separate sentence.  \n",
    "- We will read the file and store it as a **list of sentences** or a **Pandas DataFrame** for easier processing in later encoding steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Eb6sQPYPpYEr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4577,
     "status": "ok",
     "timestamp": 1759730682040,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "Eb6sQPYPpYEr",
    "outputId": "35404582-2b39-421b-977f-5681dda1f86e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m,force_remount=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3WfwSMVbZ079",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1178,
     "status": "ok",
     "timestamp": 1759730683220,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "3WfwSMVbZ079",
    "outputId": "d6f3289c-ac57-4392-d8d3-f6e27c9f6c2d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CPU dispatcher tracer already initlized",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\__init__.py:26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _dependency \u001b[38;5;129;01min\u001b[39;00m _hard_dependencies:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_dependency\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m     28\u001b[39m         _missing_dependencies.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\__init__.py:127\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _distributor_init\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__config__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_config\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    129\u001b[39m     msg = \u001b[33m\"\"\"\u001b[39m\u001b[33mError importing numpy: you should not try to import numpy from\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[33m    its source directory; please exit the numpy source tree, and relaunch\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[33m    your python interpreter from there.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\__config__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This file is generated by numpy's build process\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# It contains system_info results at the time of building this package.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     __cpu_features__,\n\u001b[32m      6\u001b[39m     __cpu_baseline__,\n\u001b[32m      7\u001b[39m     __cpu_dispatch__,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mshow_config\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m _built_with_meson = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\_core\\__init__.py:23\u001b[39m\n\u001b[32m     20\u001b[39m         env_added.append(envkey)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\_core\\multiarray.py:10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mCreate the numpy._core.multiarray namespace for backward compatibility.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mIn v1.16 the multiarray and umath c-extension modules were merged into\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\_core\\overrides.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_module\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inspect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m getargspec\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001b[32m     11\u001b[39m ARRAY_FUNCTIONS = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m     13\u001b[39m array_function_like_doc = (\n\u001b[32m     14\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"like : array_like, optional\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m        Reference object to allow the creation of arrays which are not\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[33;03m        compatible with that passed in via this argument.\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: CPU dispatcher tracer already initlized"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/Classwork_UPES/EoAIML/Classwork/Codes\")\n",
    "# Load the text data file\n",
    "with open('Text_Data.json','r') as file:\n",
    "    sentences = json.load(file)\n",
    "\n",
    "# Convert to DataFrame for easier processing\n",
    "df_text = pd.DataFrame({'Sentence': sentences})\n",
    "\n",
    "# Display first 5 sentences\n",
    "print(\"First 5 sentences in the dataset:\\n\")\n",
    "print(df_text.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QmwUzIHdqvB5",
   "metadata": {
    "id": "QmwUzIHdqvB5"
   },
   "source": [
    "# Bag of Words (BoW) Encoding\n",
    "\n",
    "**Bag of Words (BoW)** is one of the simplest and most commonly used text encoding techniques for classical machine learning.  \n",
    "\n",
    "### Concept:\n",
    "- Each sentence/document is represented as a **vector of word frequencies**.  \n",
    "- The **order of words is ignored**; only the presence or frequency matters.  \n",
    "- Creates a **vocabulary** of all unique words across the dataset.  \n",
    "\n",
    "### Representation:\n",
    "1. Let the dataset have **N sentences**:  \n",
    "   $$\n",
    "   D = \\{S_1, S_2, ..., S_N\\}\n",
    "   $$  \n",
    "2. Construct the **vocabulary V** containing all unique words:  \n",
    "   $$\n",
    "   V = \\{w_1, w_2, ..., w_m\\}\n",
    "   $$  \n",
    "   where $m$ = total number of unique words.  \n",
    "3. For each sentence $S_i$, represent it as a **vector of length m**:  \n",
    "   $$\n",
    "   \\mathbf{v}_i = [f(w_1, S_i), f(w_2, S_i), ..., f(w_m, S_i)]\n",
    "   $$  \n",
    "   where $f(w_j, S_i)$ = frequency of word $w_j$ in sentence $S_i$.  \n",
    "\n",
    "### Key Points:\n",
    "- **Sparse Representation:** Most sentences use only a small subset of vocabulary → many zeros.  \n",
    "- **Simple & interpretable:** Easy to implement and visualize.  \n",
    "- **Limitation:** Does **not capture word order or semantic meaning**.  \n",
    "- Often used as input to classical ML models like **Naive Bayes, Logistic Regression, SVM**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X-IHGKwjqxlY",
   "metadata": {
    "id": "X-IHGKwjqxlY"
   },
   "source": [
    "# Example\n",
    "\n",
    "Consider 5 sentences:\n",
    "\n",
    "1. \"I love NLP\"  \n",
    "2. \"NLP loves me\"  \n",
    "3. \"I love machine learning\"  \n",
    "4. \"Machine learning is fun fun\"   ← \"fun\" appears twice  \n",
    "5. \"I love love NLP NLP NLP\"       ← \"love\" appears twice, \"NLP\" appears thrice\n",
    "\n",
    "### Step 1: Build Vocabulary\n",
    "List all unique words (after lowercasing):  \n",
    "V = {I, love, NLP, loves, me, machine, learning, is, fun}\n",
    "\n",
    "### Step 2: BoW Vectors (frequency counts)\n",
    "\n",
    "| Sentence                    | I | love | NLP | loves | me | machine | learning | is | fun |\n",
    "|------------------------------|---|------|-----|-------|----|---------|----------|----|-----|\n",
    "| I love NLP                   | 1 | 1    | 1   | 0     | 0  | 0       | 0        | 0  | 0   |\n",
    "| NLP loves me                 | 0 | 0    | 1   | 1     | 1  | 0       | 0        | 0  | 0   |\n",
    "| I love machine learning      | 1 | 1    | 0   | 0     | 0  | 1       | 1        | 0  | 0   |\n",
    "| Machine learning is fun fun  | 0 | 0    | 0   | 0     | 0  | 1       | 1        | 1  | 2   |\n",
    "| I love love NLP NLP NLP      | 1 | 2    | 3   | 0     | 0  | 0       | 0        | 0  | 0   |\n",
    "\n",
    "### Notes:\n",
    "- Each **row** represents a sentence vector.  \n",
    "- Each **column** represents a word in the vocabulary.  \n",
    "- Repeated words are counted multiple times → reflected in frequency.  \n",
    "- This is the **core idea of Bag of Words**: frequency-based numeric representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YmzCw008q7Vr",
   "metadata": {
    "id": "YmzCw008q7Vr"
   },
   "source": [
    "# Algorithm\n",
    "\n",
    "1. **Input:** Collection of sentences/documents.\n",
    "2. **Preprocessing:** Lowercase, remove punctuation, tokenize words.\n",
    "3. **Build Vocabulary:** List all unique words in the dataset.\n",
    "4. **Vectorization:** For each sentence/document:\n",
    "   - Initialize a vector of zeros of length = vocabulary size.\n",
    "   - Count the frequency of each word in the sentence and update the vector.\n",
    "5. **Output:** Matrix of size (number of sentences × vocabulary size) representing BoW features.\n",
    "\n",
    "**Notes:**\n",
    "- Optional: Apply **stopword removal** to reduce irrelevant words.\n",
    "- Optional: Limit vocabulary to top-K frequent words to reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LP6eMrVRraec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3222,
     "status": "ok",
     "timestamp": 1759730686446,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "LP6eMrVRraec",
    "outputId": "fc72f664-10c0-4dc6-bb16-e6f31094c41b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words feature matrix (first 5 rows):\n",
      "\n",
      "   about  above  abreast  abreastly  abundant  abuts  accessed  accommodate  \\\n",
      "0      0      0        0          0         0      0         0            0   \n",
      "1      0      0        0          0         0      0         0            0   \n",
      "2      0      0        0          0         0      0         0            0   \n",
      "3      0      0        0          0         0      0         0            0   \n",
      "4      0      0        0          0         0      0         0            0   \n",
      "\n",
      "   accompanied  accompanying  ...  yards  years  yellow  yellowish  yet  you  \\\n",
      "0            0             0  ...      0      0       0          0    0    0   \n",
      "1            0             0  ...      0      0       0          0    0    0   \n",
      "2            0             0  ...      0      0       0          0    0    0   \n",
      "3            0             0  ...      0      0       0          0    0    0   \n",
      "4            0             0  ...      0      0       0          0    0    0   \n",
      "\n",
      "   zigzag  zipper  zone  zones  \n",
      "0       0       0     0      0  \n",
      "1       0       0     1      0  \n",
      "2       0       0     0      0  \n",
      "3       0       0     0      0  \n",
      "4       0       0     0      0  \n",
      "\n",
      "[5 rows x 2525 columns]\n",
      "\n",
      "Vocabulary (words as columns):\n",
      "\n",
      "['about' 'above' 'abreast' ... 'zipper' 'zone' 'zones']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the sentences\n",
    "bow_matrix = vectorizer.fit_transform(df_text['Sentence'])\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "df_bow = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display first 5 rows of BoW representation\n",
    "print(\"Bag of Words feature matrix (first 5 rows):\\n\")\n",
    "print(df_bow.head())\n",
    "\n",
    "# Display the vocabulary\n",
    "print(\"\\nVocabulary (words as columns):\\n\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nN-Iy3kXr5Ck",
   "metadata": {
    "id": "nN-Iy3kXr5Ck"
   },
   "outputs": [],
   "source": [
    "# Scratch Code\n",
    "\n",
    "class BagOfWords:\n",
    "    \"\"\"\n",
    "    A class to implement Bag of Words (BoW) encoding from scratch.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    sentences : list of str\n",
    "        List of sentences/documents to encode.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit_transform()\n",
    "        Learns vocabulary from sentences and returns the BoW feature matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences):\n",
    "        \"\"\"\n",
    "        Initialize the BagOfWords encoder with sentences.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        sentences : list of str\n",
    "            List of sentences/documents to encode.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.vocab = []\n",
    "\n",
    "    def fit_transform(self):\n",
    "        \"\"\"\n",
    "        Convert the stored sentences into a Bag of Words feature matrix.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        df_bow : pandas.DataFrame\n",
    "            DataFrame where each row represents a sentence and each column a word from vocabulary.\n",
    "        \"\"\"\n",
    "        # Step 1: Tokenize sentences and build vocabulary\n",
    "        tokenized_sentences = [sentence.lower().split() for sentence in self.sentences]\n",
    "        self.vocab = sorted(set(word for sentence in tokenized_sentences for word in sentence))\n",
    "\n",
    "        # Step 2: Initialize matrix\n",
    "        import numpy as np\n",
    "        matrix = np.zeros((len(self.sentences), len(self.vocab)), dtype=int)\n",
    "\n",
    "        # Step 3: Count word frequencies\n",
    "        for i, sentence in enumerate(tokenized_sentences):\n",
    "            for word in sentence:\n",
    "                j = self.vocab.index(word)\n",
    "                matrix[i, j] += 1\n",
    "\n",
    "        # Step 4: Convert to DataFrame\n",
    "        import pandas as pd\n",
    "        df_bow = pd.DataFrame(matrix, columns=self.vocab)\n",
    "\n",
    "        return df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fqpM4MesI84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1759730686533,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "8fqpM4MesI84",
    "outputId": "04794127-5a97-4aa0-8478-57a37d322677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words feature matrix for small example:\n",
      "\n",
      "   fun  i  is  learning  love  loves  machine  me  nlp\n",
      "0    0  1   0         0     1      0        0   0    1\n",
      "1    0  0   0         0     0      1        0   1    1\n",
      "2    0  1   0         1     1      0        1   0    0\n",
      "3    2  0   1         1     0      0        1   0    0\n",
      "4    0  1   0         0     2      0        0   0    3\n"
     ]
    }
   ],
   "source": [
    "# Small sentence set (from previous BoW example)\n",
    "sentences_example = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP loves me\",\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is fun fun\",   # 'fun' repeated twice\n",
    "    \"I love love NLP NLP NLP\"        # 'love' twice, 'NLP' thrice\n",
    "]\n",
    "\n",
    "# Initialize BagOfWords object\n",
    "bow_encoder_example = BagOfWords(sentences_example)\n",
    "\n",
    "# Generate BoW matrix\n",
    "df_bow_example = bow_encoder_example.fit_transform()\n",
    "\n",
    "# Display BoW matrix\n",
    "print(\"Bag of Words feature matrix for small example:\\n\")\n",
    "print(df_bow_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lLuMgYCysg-i",
   "metadata": {
    "id": "lLuMgYCysg-i"
   },
   "source": [
    "# TF-IDF (Term Frequency–Inverse Document Frequency) – Theory\n",
    "\n",
    "TF-IDF is a **weighted version of Bag of Words** that emphasizes important words in a document while reducing the impact of common words.\n",
    "\n",
    "### Components:\n",
    "\n",
    "1. **Term Frequency (TF)**  \n",
    "   Measures how often a term appears in a document:\n",
    "   $$\n",
    "   TF_{t,d} = \\frac{\\text{Number of times term t appears in document d}}{\\text{Total number of terms in document d}}\n",
    "   $$\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**  \n",
    "   Measures how unique a term is across all documents:\n",
    "   $$\n",
    "   IDF_t = \\log \\frac{N}{1 + \\text{Number of documents containing term t}}\n",
    "   $$  \n",
    "   - $N$ = total number of documents  \n",
    "   - Common words across many documents have lower IDF\n",
    "\n",
    "3. **TF-IDF Score**  \n",
    "   Combines TF and IDF:\n",
    "   $$\n",
    "   TF\\text{-}IDF_{t,d} = TF_{t,d} \\times IDF_t\n",
    "   $$\n",
    "\n",
    "### Key Points:\n",
    "- Words that appear frequently in a document but rarely across documents get **high scores**.  \n",
    "- Reduces weight of stopwords like \"the\", \"is\", \"and\".  \n",
    "- Output is a **sparse numeric vector**, one per document.  \n",
    "- Widely used in **text classification, clustering, and information retrieval**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uy8oXncaslPv",
   "metadata": {
    "id": "Uy8oXncaslPv"
   },
   "source": [
    "# Example\n",
    "\n",
    "Consider 3 sentences:\n",
    "\n",
    "1. \"I love NLP\"  \n",
    "2. \"NLP loves me\"  \n",
    "3. \"I love machine learning\"\n",
    "\n",
    "### Step 1: Bag of Words Frequencies\n",
    "| Sentence           | I | love | NLP | loves | me | machine | learning |\n",
    "|-------------------|---|------|-----|-------|----|---------|----------|\n",
    "| I love NLP         | 1 | 1    | 1   | 0     | 0  | 0       | 0        |\n",
    "| NLP loves me       | 0 | 0    | 1   | 1     | 1  | 0       | 0        |\n",
    "| I love machine learning | 1 | 1 | 0   | 0     | 0  | 1       | 1        |\n",
    "\n",
    "### Step 2: Compute TF-IDF\n",
    "- TF is normalized term frequency per document.  \n",
    "- IDF downweights common words (\"I\", \"love\", \"NLP\") that appear in multiple sentences.  \n",
    "- TF-IDF vectors reflect **importance of each word in context**:\n",
    "\n",
    "| Sentence           | I | love | NLP | loves | me | machine | learning |\n",
    "|-------------------|---|------|-----|-------|----|---------|----------|\n",
    "| I love NLP         | 0.41 | 0.41 | 0.41 | 0   | 0  | 0       | 0        |\n",
    "| NLP loves me       | 0   | 0    | 0.41 | 0.58 | 0.58 | 0     | 0        |\n",
    "| I love machine learning | 0.41 | 0.41 | 0   | 0   | 0 | 0.58    | 0.58     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eYPiCklpstzR",
   "metadata": {
    "id": "eYPiCklpstzR"
   },
   "source": [
    "# TF-IDF Algorithm\n",
    "\n",
    "1. **Input:** Collection of sentences/documents.  \n",
    "2. **Preprocessing:** Lowercase, remove punctuation, tokenize.  \n",
    "3. **Build Vocabulary:** List all unique words across documents.  \n",
    "4. **Compute Term Frequency (TF):** Count occurrences of each word in each document and normalize by document length.  \n",
    "5. **Compute Inverse Document Frequency (IDF):**  \n",
    "   $$\n",
    "   IDF_t = \\log \\frac{N}{1 + df_t}\n",
    "   $$  \n",
    "   where $df_t$ = number of documents containing word $t$, and $N$ = total number of documents.  \n",
    "6. **Compute TF-IDF:** Multiply TF by IDF for each term:  \n",
    "   $$\n",
    "   TF\\text{-}IDF_{t,d} = TF_{t,d} \\times IDF_t\n",
    "   $$  \n",
    "7. **Output:** Matrix of size (number of documents × vocabulary size) representing TF-IDF features.  \n",
    "\n",
    "**Notes:**  \n",
    "- Can use libraries like `TfidfVectorizer` from scikit-learn for implementation.  \n",
    "- Optional: Remove stopwords or limit vocabulary size to reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lpuglabesrg6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1625,
     "status": "ok",
     "timestamp": 1759730688160,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "Lpuglabesrg6",
    "outputId": "a429534b-4c51-4cd4-dca8-ec913db87b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature matrix (first 5 rows):\n",
      "\n",
      "   about  above  abreast  abreastly  abundant  abuts  accessed  accommodate  \\\n",
      "0    0.0    0.0      0.0        0.0       0.0    0.0       0.0          0.0   \n",
      "1    0.0    0.0      0.0        0.0       0.0    0.0       0.0          0.0   \n",
      "2    0.0    0.0      0.0        0.0       0.0    0.0       0.0          0.0   \n",
      "3    0.0    0.0      0.0        0.0       0.0    0.0       0.0          0.0   \n",
      "4    0.0    0.0      0.0        0.0       0.0    0.0       0.0          0.0   \n",
      "\n",
      "   accompanied  accompanying  ...  yards  years  yellow  yellowish  yet  you  \\\n",
      "0          0.0           0.0  ...    0.0    0.0     0.0        0.0  0.0  0.0   \n",
      "1          0.0           0.0  ...    0.0    0.0     0.0        0.0  0.0  0.0   \n",
      "2          0.0           0.0  ...    0.0    0.0     0.0        0.0  0.0  0.0   \n",
      "3          0.0           0.0  ...    0.0    0.0     0.0        0.0  0.0  0.0   \n",
      "4          0.0           0.0  ...    0.0    0.0     0.0        0.0  0.0  0.0   \n",
      "\n",
      "   zigzag  zipper      zone  zones  \n",
      "0     0.0     0.0  0.000000    0.0  \n",
      "1     0.0     0.0  0.612699    0.0  \n",
      "2     0.0     0.0  0.000000    0.0  \n",
      "3     0.0     0.0  0.000000    0.0  \n",
      "4     0.0     0.0  0.000000    0.0  \n",
      "\n",
      "[5 rows x 2525 columns]\n",
      "\n",
      "Vocabulary (words as columns):\n",
      "\n",
      "['about' 'above' 'abreast' ... 'zipper' 'zone' 'zones']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the sentences\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_text['Sentence'])\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display first 5 rows of TF-IDF feature matrix\n",
    "print(\"TF-IDF feature matrix (first 5 rows):\\n\")\n",
    "print(df_tfidf.head())\n",
    "\n",
    "# Display the vocabulary\n",
    "print(\"\\nVocabulary (words as columns):\\n\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ETU7zyIds7Tk",
   "metadata": {
    "id": "ETU7zyIds7Tk"
   },
   "outputs": [],
   "source": [
    "# Scratch code\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TFIDF:\n",
    "    \"\"\"\n",
    "    A class to implement TF-IDF encoding from scratch.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    sentences : list of str\n",
    "        List of sentences/documents to encode.\n",
    "    vocab : list of str\n",
    "        Unique words across all sentences.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit_transform()\n",
    "        Computes the TF-IDF feature matrix for the stored sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences):\n",
    "        \"\"\"\n",
    "        Initialize the TFIDF encoder with sentences.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        sentences : list of str\n",
    "            List of sentences/documents to encode.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.vocab = []\n",
    "\n",
    "    def fit_transform(self):\n",
    "        \"\"\"\n",
    "        Compute TF-IDF feature matrix from the stored sentences.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        df_tfidf : pandas.DataFrame\n",
    "            DataFrame where each row represents a sentence and each column a word from vocabulary.\n",
    "        \"\"\"\n",
    "        # Step 1: Tokenize sentences\n",
    "        tokenized_sentences = [sentence.lower().split() for sentence in self.sentences]\n",
    "\n",
    "        # Step 2: Build vocabulary\n",
    "        self.vocab = sorted(set(word for sentence in tokenized_sentences for word in sentence))\n",
    "\n",
    "        N = len(tokenized_sentences)  # number of documents\n",
    "\n",
    "        # Step 3: Compute document frequency for each word\n",
    "        df_count = {word: 0 for word in self.vocab}\n",
    "        for word in self.vocab:\n",
    "            df_count[word] = sum(word in sentence for sentence in tokenized_sentences)\n",
    "\n",
    "        # Step 4: Initialize TF-IDF matrix\n",
    "        tfidf_matrix = np.zeros((N, len(self.vocab)))\n",
    "\n",
    "        # Step 5: Compute TF-IDF values\n",
    "        for i, sentence in enumerate(tokenized_sentences):\n",
    "            total_terms = len(sentence)\n",
    "            for j, word in enumerate(self.vocab):\n",
    "                tf = sentence.count(word) / total_terms  # term frequency\n",
    "                idf = np.log(N / (1 + df_count[word]))   # inverse document frequency\n",
    "                tfidf_matrix[i, j] = tf * idf\n",
    "\n",
    "        # Step 6: Convert to DataFrame\n",
    "        df_tfidf = pd.DataFrame(tfidf_matrix, columns=self.vocab)\n",
    "        return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "REeUTeQctWHe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1759730688222,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "REeUTeQctWHe",
    "outputId": "d94692cc-cca9-4631-bf72-83781eecf459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature matrix for small example:\n",
      "\n",
      "        fun         i        is  learning      love    loves   machine  \\\n",
      "0  0.000000  0.074381  0.000000  0.000000  0.074381  0.00000  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.30543  0.000000   \n",
      "2  0.000000  0.055786  0.000000  0.127706  0.055786  0.00000  0.127706   \n",
      "3  0.366516  0.000000  0.183258  0.102165  0.000000  0.00000  0.102165   \n",
      "4  0.000000  0.037191  0.000000  0.000000  0.074381  0.00000  0.000000   \n",
      "\n",
      "        me       nlp  \n",
      "0  0.00000  0.074381  \n",
      "1  0.30543  0.074381  \n",
      "2  0.00000  0.000000  \n",
      "3  0.00000  0.000000  \n",
      "4  0.00000  0.111572  \n"
     ]
    }
   ],
   "source": [
    "# Small 5-sentence example\n",
    "sentences_example = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP loves me\",\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is fun fun\",\n",
    "    \"I love love NLP NLP NLP\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF object\n",
    "tfidf_encoder = TFIDF(sentences_example)\n",
    "\n",
    "# Generate TF-IDF matrix\n",
    "df_tfidf_example = tfidf_encoder.fit_transform()\n",
    "\n",
    "# Display TF-IDF matrix\n",
    "print(\"TF-IDF feature matrix for small example:\\n\")\n",
    "print(df_tfidf_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4lgrO4rhx2sl",
   "metadata": {
    "id": "4lgrO4rhx2sl"
   },
   "source": [
    "# Word2Vec – Word Embeddings\n",
    "\n",
    "**Word2Vec** is a neural network-based model that converts words into dense vectors of fixed size, capturing semantic meaning and relationships between words.\n",
    "\n",
    "### Key Concepts:\n",
    "- Each word is represented as a **dense vector** in a high-dimensional space.\n",
    "- Words appearing in **similar contexts** have similar vectors.\n",
    "- Captures semantic relationships, e.g.,\n",
    "  $$\n",
    "  \\text{vec}(king) - \\text{vec}(man) + \\text{vec}(woman) \\approx \\text{vec}(queen)\n",
    "  $$\n",
    "\n",
    "### Models:\n",
    "1. **CBOW (Continuous Bag of Words):**\n",
    "   - Predicts a target word from surrounding context words.\n",
    "   - Faster and works well with smaller datasets.\n",
    "\n",
    "2. **Skip-gram:**\n",
    "   - Predicts surrounding context words given a target word.\n",
    "   - Performs better with rare words.\n",
    "\n",
    "### Key Points:\n",
    "- Output is a **dense vector** for each word (usually 100–300 dimensions).\n",
    "- Preserves **semantic similarity** between words.\n",
    "- Commonly used in NLP tasks: sentiment analysis, text classification, recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zFTrNE7wy8wr",
   "metadata": {
    "id": "zFTrNE7wy8wr"
   },
   "source": [
    "# Word2Vec Example\n",
    "\n",
    "Sentences:\n",
    "1. \"I love NLP\"\n",
    "2. \"NLP loves me\"\n",
    "3. \"I love machine learning\"\n",
    "\n",
    "- Word2Vec will create a **vector representation** for each word.\n",
    "- Similar words in context get **closer vectors**.\n",
    "- Example: After training a 5-dimensional Word2Vec:\n",
    "  $$\n",
    "  \\text{vec}(NLP) = [0.12, -0.05, 0.33, 0.21, 0.08]\n",
    "  $$\n",
    "  $$\n",
    "  \\text{vec}(love) = [0.25, 0.11, 0.30, -0.02, 0.15]\n",
    "  $$\n",
    "- Vectors can then be used as features for ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mnppb8lgzUVc",
   "metadata": {
    "id": "Mnppb8lgzUVc"
   },
   "source": [
    "# Word2Vec Algorithm\n",
    "\n",
    "1. **Input:** Collection of sentences/documents.  \n",
    "2. **Preprocessing:** Tokenize sentences, lowercase words, remove punctuation.  \n",
    "3. **Choose model:** CBOW or Skip-gram.  \n",
    "4. **Train Neural Network:** Learn dense vector representations for each word.  \n",
    "5. **Output:** Dictionary mapping each word to its vector representation.\n",
    "\n",
    "**Notes:**\n",
    "- Libraries: `gensim` (Word2Vec), `tensorflow` or `pytorch` for custom training.\n",
    "- Hyperparameters: vector size, window size, min_count, epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8IZ57lLozmhR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13659,
     "status": "ok",
     "timestamp": 1759730701883,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "8IZ57lLozmhR",
    "outputId": "c969e368-53f5-4ca5-e5f2-4d179ed119cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "# install gensim\n",
    "# !pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DK_9Qjo1zShN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36073,
     "status": "ok",
     "timestamp": 1759730737963,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "DK_9Qjo1zShN",
    "outputId": "9c1cb221-e742-423c-f377-8ae9bdb10657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation for 'buildings':\n",
      " [-0.24368595  0.67628956 -0.07799989 ...  0.07771645  0.08644833\n",
      "  0.16326168]\n",
      "\n",
      "Top 5 words similar to 'buildings':\n",
      " [('building', 0.5575284361839294), ('houses', 0.556564211845398), ('meadows', 0.3854692876338959), ('trees', 0.3841174244880676), ('green', 0.3694857954978943)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "# Tokenize sentences (split into words)\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "# CBOW: sg=0 (default), Skip-gram: sg=1\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,  # Tokenized corpus: list of list of words\n",
    "    vector_size=1024,               # Dimensionality of the word embeddings\n",
    "    window=10,                      # Context window size\n",
    "    min_count=1,                    # Include all words (min frequency threshold)\n",
    "    sg=0,                           # Training algorithm: 0 = CBOW, 1 = Skip-gram\n",
    "    workers=4,                      # Number of CPU cores for training\n",
    "    epochs=50                       # Number of training iterations over the corpus\n",
    ")\n",
    "\n",
    "\n",
    "# Example: Get vector for a word\n",
    "word = \"buildings\"\n",
    "if word in w2v_model.wv:\n",
    "    print(f\"Vector representation for '{word}':\\n\", w2v_model.wv.get_vector(word))\n",
    "\n",
    "# Example: Most similar words\n",
    "similar_words = w2v_model.wv.most_similar(word, topn=5)\n",
    "print(f\"\\nTop 5 words similar to '{word}':\\n\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JQEeOyAu1N8H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1759730738091,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "JQEeOyAu1N8H",
    "outputId": "7c1dbd44-01f5-40e6-cc22-1e5cc53ac88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vector representation for 'coding':\t (100,)\n",
      "Shape of vector representation for 'fun':\t (100,)\n",
      "Shape of vector representation for 'i':\t (100,)\n",
      "Shape of vector representation for 'is':\t (100,)\n",
      "Shape of vector representation for 'learning':\t (100,)\n",
      "Shape of vector representation for 'like':\t (100,)\n",
      "Shape of vector representation for 'likes':\t (100,)\n",
      "Shape of vector representation for 'love':\t (100,)\n",
      "Shape of vector representation for 'loves':\t (100,)\n",
      "Shape of vector representation for 'machine':\t (100,)\n",
      "Shape of vector representation for 'me':\t (100,)\n",
      "Shape of vector representation for 'nlp':\t (100,)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "# Small sentence set for demonstration\n",
    "csentences = [\n",
    "    \"I love NLP\",\n",
    "    \"I like NLP\",\n",
    "    \"NLP loves me\",\n",
    "    \"NLP likes me\",\n",
    "    \"I love machine learning\",\n",
    "    \"I like machine learning\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"I love coding\",\n",
    "    \"I like coding\",\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [s.lower().split() for s in csentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "# CBOW: sg=0 (default), Skip-gram: sg=1\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,  # Tokenized corpus: list of list of words\n",
    "    vector_size=100,               # Dimensionality of the word embeddings\n",
    "    window=10,                      # Context window size\n",
    "    min_count=1,                    # Include all words (min frequency threshold)\n",
    "    sg=0,                           # Training algorithm: 0 = CBOW, 1 = Skip-gram\n",
    "    workers=4,                      # Number of CPU cores for training\n",
    "    epochs=50                       # Number of training iterations over the corpus\n",
    ")\n",
    "\n",
    "# Vocabulary\n",
    "vocabulary = sorted(w2v_model.wv.index_to_key)  # all words in sorted order\n",
    "\n",
    "# Example: Get vector for all words\n",
    "for word in vocabulary:\n",
    "    print(f\"Shape of vector representation for '{word}':\\t\", w2v_model.wv.get_vector(word).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SeP_21rC490l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1759730738200,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "SeP_21rC490l",
    "outputId": "840b2e6f-e9c3-4c18-b587-1dd33793f49e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"Rank_1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"me\",\n          \"machine\",\n          \"coding\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"learning\",\n          \"is\",\n          \"love\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"likes\",\n          \"machine\",\n          \"love\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"me\",\n          \"love\",\n          \"coding\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"like\",\n          \"is\",\n          \"learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_6\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"love\",\n          \"loves\",\n          \"like\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_7\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"nlp\",\n          \"love\",\n          \"i\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_8\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"learning\",\n          \"i\",\n          \"is\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_9\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"me\",\n          \"likes\",\n          \"love\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_10\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"is\",\n          \"coding\",\n          \"me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_11\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"like\",\n          \"is\",\n          \"loves\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_12\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"me\",\n          \"is\",\n          \"nlp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-92580cb1-4d95-4cb2-b9db-4a7be8ad7214\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank_1</th>\n",
       "      <th>Rank_2</th>\n",
       "      <th>Rank_3</th>\n",
       "      <th>Rank_4</th>\n",
       "      <th>Rank_5</th>\n",
       "      <th>Rank_6</th>\n",
       "      <th>Rank_7</th>\n",
       "      <th>Rank_8</th>\n",
       "      <th>Rank_9</th>\n",
       "      <th>Rank_10</th>\n",
       "      <th>Rank_11</th>\n",
       "      <th>Rank_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coding</th>\n",
       "      <td>coding</td>\n",
       "      <td>learning</td>\n",
       "      <td>likes</td>\n",
       "      <td>machine</td>\n",
       "      <td>like</td>\n",
       "      <td>love</td>\n",
       "      <td>i</td>\n",
       "      <td>fun</td>\n",
       "      <td>me</td>\n",
       "      <td>is</td>\n",
       "      <td>loves</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fun</th>\n",
       "      <td>fun</td>\n",
       "      <td>learning</td>\n",
       "      <td>machine</td>\n",
       "      <td>love</td>\n",
       "      <td>is</td>\n",
       "      <td>loves</td>\n",
       "      <td>nlp</td>\n",
       "      <td>i</td>\n",
       "      <td>likes</td>\n",
       "      <td>coding</td>\n",
       "      <td>like</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>i</td>\n",
       "      <td>is</td>\n",
       "      <td>me</td>\n",
       "      <td>loves</td>\n",
       "      <td>likes</td>\n",
       "      <td>fun</td>\n",
       "      <td>coding</td>\n",
       "      <td>nlp</td>\n",
       "      <td>like</td>\n",
       "      <td>learning</td>\n",
       "      <td>love</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>is</td>\n",
       "      <td>i</td>\n",
       "      <td>fun</td>\n",
       "      <td>likes</td>\n",
       "      <td>coding</td>\n",
       "      <td>learning</td>\n",
       "      <td>loves</td>\n",
       "      <td>me</td>\n",
       "      <td>nlp</td>\n",
       "      <td>love</td>\n",
       "      <td>machine</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>learning</td>\n",
       "      <td>fun</td>\n",
       "      <td>loves</td>\n",
       "      <td>like</td>\n",
       "      <td>coding</td>\n",
       "      <td>love</td>\n",
       "      <td>me</td>\n",
       "      <td>machine</td>\n",
       "      <td>nlp</td>\n",
       "      <td>is</td>\n",
       "      <td>i</td>\n",
       "      <td>likes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>like</td>\n",
       "      <td>learning</td>\n",
       "      <td>love</td>\n",
       "      <td>coding</td>\n",
       "      <td>nlp</td>\n",
       "      <td>loves</td>\n",
       "      <td>i</td>\n",
       "      <td>me</td>\n",
       "      <td>machine</td>\n",
       "      <td>fun</td>\n",
       "      <td>is</td>\n",
       "      <td>likes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>likes</th>\n",
       "      <td>likes</td>\n",
       "      <td>loves</td>\n",
       "      <td>me</td>\n",
       "      <td>i</td>\n",
       "      <td>coding</td>\n",
       "      <td>fun</td>\n",
       "      <td>love</td>\n",
       "      <td>is</td>\n",
       "      <td>machine</td>\n",
       "      <td>learning</td>\n",
       "      <td>nlp</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>love</td>\n",
       "      <td>loves</td>\n",
       "      <td>like</td>\n",
       "      <td>machine</td>\n",
       "      <td>learning</td>\n",
       "      <td>fun</td>\n",
       "      <td>likes</td>\n",
       "      <td>coding</td>\n",
       "      <td>nlp</td>\n",
       "      <td>me</td>\n",
       "      <td>i</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loves</th>\n",
       "      <td>loves</td>\n",
       "      <td>learning</td>\n",
       "      <td>love</td>\n",
       "      <td>me</td>\n",
       "      <td>likes</td>\n",
       "      <td>i</td>\n",
       "      <td>machine</td>\n",
       "      <td>fun</td>\n",
       "      <td>nlp</td>\n",
       "      <td>like</td>\n",
       "      <td>is</td>\n",
       "      <td>coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>machine</td>\n",
       "      <td>love</td>\n",
       "      <td>fun</td>\n",
       "      <td>nlp</td>\n",
       "      <td>coding</td>\n",
       "      <td>loves</td>\n",
       "      <td>likes</td>\n",
       "      <td>learning</td>\n",
       "      <td>me</td>\n",
       "      <td>like</td>\n",
       "      <td>i</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>me</td>\n",
       "      <td>loves</td>\n",
       "      <td>likes</td>\n",
       "      <td>i</td>\n",
       "      <td>coding</td>\n",
       "      <td>learning</td>\n",
       "      <td>machine</td>\n",
       "      <td>like</td>\n",
       "      <td>love</td>\n",
       "      <td>is</td>\n",
       "      <td>nlp</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp</th>\n",
       "      <td>nlp</td>\n",
       "      <td>machine</td>\n",
       "      <td>fun</td>\n",
       "      <td>love</td>\n",
       "      <td>loves</td>\n",
       "      <td>like</td>\n",
       "      <td>i</td>\n",
       "      <td>learning</td>\n",
       "      <td>is</td>\n",
       "      <td>coding</td>\n",
       "      <td>me</td>\n",
       "      <td>likes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92580cb1-4d95-4cb2-b9db-4a7be8ad7214')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-92580cb1-4d95-4cb2-b9db-4a7be8ad7214 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-92580cb1-4d95-4cb2-b9db-4a7be8ad7214');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-73bd0951-3e45-4bf0-be64-165b9914c5f8\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-73bd0951-3e45-4bf0-be64-165b9914c5f8')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-73bd0951-3e45-4bf0-be64-165b9914c5f8 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_cf44b4cd-0acf-4aca-83c6-1796ac28b74d\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_cf44b4cd-0acf-4aca-83c6-1796ac28b74d button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "            Rank_1    Rank_2   Rank_3   Rank_4    Rank_5    Rank_6   Rank_7  \\\n",
       "coding      coding  learning    likes  machine      like      love        i   \n",
       "fun            fun  learning  machine     love        is     loves      nlp   \n",
       "i                i        is       me    loves     likes       fun   coding   \n",
       "is              is         i      fun    likes    coding  learning    loves   \n",
       "learning  learning       fun    loves     like    coding      love       me   \n",
       "like          like  learning     love   coding       nlp     loves        i   \n",
       "likes        likes     loves       me        i    coding       fun     love   \n",
       "love          love     loves     like  machine  learning       fun    likes   \n",
       "loves        loves  learning     love       me     likes         i  machine   \n",
       "machine    machine      love      fun      nlp    coding     loves    likes   \n",
       "me              me     loves    likes        i    coding  learning  machine   \n",
       "nlp            nlp   machine      fun     love     loves      like        i   \n",
       "\n",
       "            Rank_8   Rank_9   Rank_10  Rank_11  Rank_12  \n",
       "coding         fun       me        is    loves      nlp  \n",
       "fun              i    likes    coding     like       me  \n",
       "i              nlp     like  learning     love  machine  \n",
       "is              me      nlp      love  machine     like  \n",
       "learning   machine      nlp        is        i    likes  \n",
       "like            me  machine       fun       is    likes  \n",
       "likes           is  machine  learning      nlp     like  \n",
       "love        coding      nlp        me        i       is  \n",
       "loves          fun      nlp      like       is   coding  \n",
       "machine   learning       me      like        i       is  \n",
       "me            like     love        is      nlp      fun  \n",
       "nlp       learning       is    coding       me    likes  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Vocabulary\n",
    "vocabulary = sorted(w2v_model.wv.index_to_key)  # all words in sorted order\n",
    "\n",
    "# Build matrix of word vectors\n",
    "vectors = np.array([w2v_model.wv[word] for word in vocabulary])\n",
    "\n",
    "# Compute cosine similarity between all words\n",
    "similarity_matrix = cosine_similarity(vectors)\n",
    "\n",
    "# For each word, get words in descending similarity\n",
    "df_data = [\n",
    "    [vocabulary[idx] for idx in np.argsort(-similarity_matrix[i])]\n",
    "    for i in range(len(vocabulary))\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(df_data, index=vocabulary, columns=[f'Rank_{i+1}' for i in range(len(vocabulary))])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l510qVlimSHn",
   "metadata": {
    "id": "l510qVlimSHn"
   },
   "source": [
    "# FastText – Theory\n",
    "\n",
    "**FastText** is an extension of Word2Vec developed by Facebook AI Research (FAIR).  \n",
    "Unlike Word2Vec, which learns a vector for each *whole word*, FastText represents each word as a **bag of character n-grams**.  \n",
    "This makes it more powerful for handling rare words, misspellings, and morphologically rich languages.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "- Each word $w$ is represented as a combination of its character *n-grams*.  \n",
    "- Example: For the word *\"where\"* with $n = 3$:  \n",
    "  - n-grams = `\"<wh\"`, `\"whe\"`, `\"her\"`, `\"ere\"`, `\"re>\"`  \n",
    "- The vector for $w$ is computed as the **sum of vectors of its n-grams**.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Representation\n",
    "\n",
    "If a word $w$ is represented by character n-grams  \n",
    "$(g_{1}, g_{2}, \\ldots, g_{k})$, then its vector is:\n",
    "\n",
    "$$\n",
    "\\vec{w} = \\sum_{i=1}^{k} \\vec{g_i}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $\\vec{g_i}$ = vector of the $i^{th}$ n-gram  \n",
    "- $k$ = total number of n-grams for word $w$\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages\n",
    "- Handles **out-of-vocabulary (OOV) words** by using subword n-grams.  \n",
    "- Better at capturing **morphological variations** (e.g., \"play\", \"playing\", \"played\").  \n",
    "- Works well for **low-resource languages** and noisy text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vEEp0NzgnJih",
   "metadata": {
    "id": "vEEp0NzgnJih"
   },
   "source": [
    "# FastText – Small Example\n",
    "\n",
    "Suppose we use character 3-grams ($n=3$).  \n",
    "\n",
    "### Word: \"play\"\n",
    "- 3-grams: `\"<pl\"`, `\"pla\"`, `\"lay\"`, `\"ay>\"`  \n",
    "\n",
    "### Word: \"player\"\n",
    "- 3-grams: `\"<pl\"`, `\"pla\"`, `\"lay\"`, `\"aye\"`, `\"yer\"`, `\"er>\"`  \n",
    "\n",
    "### Key Observation\n",
    "- Both words share the n-grams `\"<pl\"`, `\"pla\"`, `\"lay\"`.  \n",
    "- This makes their vectors **similar**, even though \"play\" and \"player\" are different words.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j-J9baCSnTso",
   "metadata": {
    "id": "j-J9baCSnTso"
   },
   "source": [
    "# FastText – Algorithm\n",
    "\n",
    "1. **Input:** Collection of sentences/documents.  \n",
    "2. **Preprocessing:** Tokenize text, lowercase, remove punctuation (optional).  \n",
    "3. **Generate n-grams:** For each word, break into character n-grams of length $n$.  \n",
    "4. **Assign vectors:** Initialize random vectors for each n-gram.  \n",
    "5. **Training:**  \n",
    "   - Similar to Word2Vec (CBOW or Skip-gram).  \n",
    "   - Instead of predicting using word vectors, use the **sum of its n-gram vectors**.  \n",
    "6. **Output:** Word embeddings that are enriched by subword information.  \n",
    "\n",
    "**Notes:**  \n",
    "- In practice, use `gensim.models.FastText` for training.  \n",
    "- Choose n-gram range carefully (commonly $n = 3$ to $6$).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bht5E7rWnVjg",
   "metadata": {
    "id": "bht5E7rWnVjg"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "class FastTextEncoder:\n",
    "    \"\"\"\n",
    "    A class to generate FastText embeddings for a corpus of tokenized sentences.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    train(sentences, vector_size=100, window=5, min_count=1, sg=0, epochs=50)\n",
    "        Train the FastText model on tokenized sentences.\n",
    "\n",
    "    get_vector(word)\n",
    "        Return the embedding vector for a given word.\n",
    "\n",
    "    most_similar(word, topn=5)\n",
    "        Return the top-n most similar words to the given word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences):\n",
    "        \"\"\"\n",
    "        Initialize with tokenized sentences.\n",
    "        Parameters:\n",
    "        -----------\n",
    "        sentences : list of list of str\n",
    "            Tokenized sentences (list of words).\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, vector_size=100, window=5, min_count=1, sg=0, epochs=50):\n",
    "        \"\"\"\n",
    "        Train FastText model on the corpus.\n",
    "        \"\"\"\n",
    "        self.model = FastText(\n",
    "            sentences=self.sentences,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            sg=sg,\n",
    "            workers=4,\n",
    "            epochs=epochs\n",
    "        )\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        \"\"\"Return vector of a word.\"\"\"\n",
    "        if self.model and word in self.model.wv:\n",
    "            return self.model.wv[word]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def most_similar(self, word, topn=5):\n",
    "        \"\"\"Return top-n most similar words.\"\"\"\n",
    "        if self.model and word in self.model.wv:\n",
    "            return self.model.wv.most_similar(word, topn=topn)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RPwHnCLxnfpi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48296,
     "status": "ok",
     "timestamp": 1759730786530,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "RPwHnCLxnfpi",
    "outputId": "ed1528e3-8381-4a5d-fa8f-4860599c2ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'trees':\n",
      " [ 0.6530846   0.878089    0.22609228  1.0980083   4.0503297  -2.1577334\n",
      " -1.019514   -0.9750183   0.11853927 -1.746333    0.20947021  1.0823535\n",
      " -2.3126569  -0.25367138 -4.2428236  -0.25693595  0.29158935  0.40239558\n",
      "  1.5529388   0.14716645  1.4325798  -1.4569715  -0.5504494  -2.5085375\n",
      " -0.05441535  0.8548568  -2.7766485  -2.186073    2.923112   -0.7620086\n",
      " -2.645418   -0.54623264  0.1195144   0.21797189 -2.3204353   0.48574346\n",
      "  1.8264054   0.5877467  -0.21056727  0.18775581 -1.0848439   3.507628\n",
      "  1.7511572  -0.9868019  -0.74996454  0.29289952  1.5102203   0.6893213\n",
      "  2.2077527  -1.3493694 ]\n",
      "\n",
      "Top 5 words similar to 'trees':\n",
      " [('plants', 0.8836347460746765), ('tree', 0.7652081847190857), ('meadows', 0.7073495388031006), ('buildings', 0.665357768535614), ('plans', 0.610248327255249)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences (split into words)\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "\n",
    "# Initialize encoder with tokenized sentences\n",
    "ft_encoder = FastTextEncoder(tokenized_sentences)\n",
    "\n",
    "# Train FastText embeddings\n",
    "ft_encoder.train(vector_size=50, window=3, sg=0, epochs=50)  # CBOW, small vector for demo\n",
    "\n",
    "# Example: Get vector for a word\n",
    "word = \"trees\"\n",
    "vector = ft_encoder.get_vector(word)\n",
    "print(f\"Vector for '{word}':\\n\", vector)\n",
    "\n",
    "# Example: Most similar words\n",
    "similar_words = ft_encoder.most_similar(word, topn=5)\n",
    "print(f\"\\nTop 5 words similar to '{word}':\\n\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B0hZRBbCpI2o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2093,
     "status": "ok",
     "timestamp": 1759730788618,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "B0hZRBbCpI2o",
    "outputId": "b1292671-eab8-4890-b19b-da3bc308d1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vector representation for 'coding':\t (100,)\n",
      "Shape of vector representation for 'fun':\t (100,)\n",
      "Shape of vector representation for 'i':\t (100,)\n",
      "Shape of vector representation for 'is':\t (100,)\n",
      "Shape of vector representation for 'learning':\t (100,)\n",
      "Shape of vector representation for 'like':\t (100,)\n",
      "Shape of vector representation for 'likes':\t (100,)\n",
      "Shape of vector representation for 'love':\t (100,)\n",
      "Shape of vector representation for 'loves':\t (100,)\n",
      "Shape of vector representation for 'machine':\t (100,)\n",
      "Shape of vector representation for 'me':\t (100,)\n",
      "Shape of vector representation for 'nlp':\t (100,)\n"
     ]
    }
   ],
   "source": [
    "# Small sentence set for demonstration\n",
    "csentences = [\n",
    "    \"I love NLP\",\n",
    "    \"I like NLP\",\n",
    "    \"NLP loves me\",\n",
    "    \"NLP likes me\",\n",
    "    \"I love machine learning\",\n",
    "    \"I like machine learning\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"I love coding\",\n",
    "    \"I like coding\",\n",
    "]\n",
    "\n",
    "# Tokenize sentences (split into words)\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in csentences]\n",
    "\n",
    "# Initialize encoder with tokenized sentences\n",
    "ft_model = FastTextEncoder(tokenized_sentences)\n",
    "\n",
    "# Train FastText embeddings\n",
    "ft_model.train(vector_size=100, window=3, sg=0, epochs=50)  # CBOW, small vector for demo\n",
    "\n",
    "# Vocabulary (all words in sorted order)\n",
    "vocabulary = sorted(ft_model.model.wv.index_to_key)\n",
    "\n",
    "# Example: Get vector for all words\n",
    "for word in vocabulary:\n",
    "    print(f\"Shape of vector representation for '{word}':\\t\", ft_model.get_vector(word).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_0F3eTbbqKF4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1759730788674,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "_0F3eTbbqKF4",
    "outputId": "2a5e42c5-f33c-4f87-9cb5-f0b2db082873"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"Rank_1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"me\",\n          \"machine\",\n          \"coding\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"learning\",\n          \"like\",\n          \"loves\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"love\",\n          \"likes\",\n          \"machine\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"like\",\n          \"learning\",\n          \"nlp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"coding\",\n          \"love\",\n          \"me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_6\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"coding\",\n          \"nlp\",\n          \"love\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_7\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"nlp\",\n          \"is\",\n          \"likes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_8\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"me\",\n          \"machine\",\n          \"learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_9\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"me\",\n          \"machine\",\n          \"like\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_10\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"fun\",\n          \"loves\",\n          \"learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_11\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"is\",\n          \"i\",\n          \"loves\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rank_12\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"is\",\n          \"like\",\n          \"me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-7980cc63-9401-4dea-9442-a39c3769179e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank_1</th>\n",
       "      <th>Rank_2</th>\n",
       "      <th>Rank_3</th>\n",
       "      <th>Rank_4</th>\n",
       "      <th>Rank_5</th>\n",
       "      <th>Rank_6</th>\n",
       "      <th>Rank_7</th>\n",
       "      <th>Rank_8</th>\n",
       "      <th>Rank_9</th>\n",
       "      <th>Rank_10</th>\n",
       "      <th>Rank_11</th>\n",
       "      <th>Rank_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coding</th>\n",
       "      <td>coding</td>\n",
       "      <td>learning</td>\n",
       "      <td>is</td>\n",
       "      <td>loves</td>\n",
       "      <td>machine</td>\n",
       "      <td>love</td>\n",
       "      <td>likes</td>\n",
       "      <td>i</td>\n",
       "      <td>like</td>\n",
       "      <td>fun</td>\n",
       "      <td>nlp</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fun</th>\n",
       "      <td>fun</td>\n",
       "      <td>like</td>\n",
       "      <td>likes</td>\n",
       "      <td>learning</td>\n",
       "      <td>love</td>\n",
       "      <td>coding</td>\n",
       "      <td>nlp</td>\n",
       "      <td>machine</td>\n",
       "      <td>me</td>\n",
       "      <td>loves</td>\n",
       "      <td>i</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>i</td>\n",
       "      <td>likes</td>\n",
       "      <td>nlp</td>\n",
       "      <td>loves</td>\n",
       "      <td>is</td>\n",
       "      <td>like</td>\n",
       "      <td>learning</td>\n",
       "      <td>coding</td>\n",
       "      <td>love</td>\n",
       "      <td>machine</td>\n",
       "      <td>me</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>is</td>\n",
       "      <td>love</td>\n",
       "      <td>coding</td>\n",
       "      <td>me</td>\n",
       "      <td>likes</td>\n",
       "      <td>i</td>\n",
       "      <td>loves</td>\n",
       "      <td>nlp</td>\n",
       "      <td>learning</td>\n",
       "      <td>machine</td>\n",
       "      <td>like</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>learning</td>\n",
       "      <td>coding</td>\n",
       "      <td>i</td>\n",
       "      <td>loves</td>\n",
       "      <td>fun</td>\n",
       "      <td>like</td>\n",
       "      <td>likes</td>\n",
       "      <td>is</td>\n",
       "      <td>love</td>\n",
       "      <td>nlp</td>\n",
       "      <td>machine</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>like</td>\n",
       "      <td>likes</td>\n",
       "      <td>machine</td>\n",
       "      <td>fun</td>\n",
       "      <td>me</td>\n",
       "      <td>i</td>\n",
       "      <td>love</td>\n",
       "      <td>learning</td>\n",
       "      <td>nlp</td>\n",
       "      <td>coding</td>\n",
       "      <td>loves</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>likes</th>\n",
       "      <td>likes</td>\n",
       "      <td>like</td>\n",
       "      <td>me</td>\n",
       "      <td>loves</td>\n",
       "      <td>i</td>\n",
       "      <td>fun</td>\n",
       "      <td>is</td>\n",
       "      <td>love</td>\n",
       "      <td>machine</td>\n",
       "      <td>learning</td>\n",
       "      <td>coding</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>love</td>\n",
       "      <td>loves</td>\n",
       "      <td>is</td>\n",
       "      <td>machine</td>\n",
       "      <td>coding</td>\n",
       "      <td>nlp</td>\n",
       "      <td>likes</td>\n",
       "      <td>like</td>\n",
       "      <td>fun</td>\n",
       "      <td>me</td>\n",
       "      <td>i</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loves</th>\n",
       "      <td>loves</td>\n",
       "      <td>love</td>\n",
       "      <td>coding</td>\n",
       "      <td>nlp</td>\n",
       "      <td>likes</td>\n",
       "      <td>i</td>\n",
       "      <td>machine</td>\n",
       "      <td>learning</td>\n",
       "      <td>is</td>\n",
       "      <td>me</td>\n",
       "      <td>fun</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>machine</td>\n",
       "      <td>like</td>\n",
       "      <td>love</td>\n",
       "      <td>coding</td>\n",
       "      <td>nlp</td>\n",
       "      <td>likes</td>\n",
       "      <td>loves</td>\n",
       "      <td>me</td>\n",
       "      <td>fun</td>\n",
       "      <td>learning</td>\n",
       "      <td>is</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>me</td>\n",
       "      <td>likes</td>\n",
       "      <td>nlp</td>\n",
       "      <td>like</td>\n",
       "      <td>is</td>\n",
       "      <td>machine</td>\n",
       "      <td>love</td>\n",
       "      <td>loves</td>\n",
       "      <td>fun</td>\n",
       "      <td>coding</td>\n",
       "      <td>i</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp</th>\n",
       "      <td>nlp</td>\n",
       "      <td>me</td>\n",
       "      <td>loves</td>\n",
       "      <td>love</td>\n",
       "      <td>machine</td>\n",
       "      <td>i</td>\n",
       "      <td>like</td>\n",
       "      <td>is</td>\n",
       "      <td>fun</td>\n",
       "      <td>coding</td>\n",
       "      <td>learning</td>\n",
       "      <td>likes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7980cc63-9401-4dea-9442-a39c3769179e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-7980cc63-9401-4dea-9442-a39c3769179e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-7980cc63-9401-4dea-9442-a39c3769179e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-89a25847-0b59-4769-844b-41bf8c390d6b\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-89a25847-0b59-4769-844b-41bf8c390d6b')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-89a25847-0b59-4769-844b-41bf8c390d6b button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_e159f60f-6686-4ad3-ad6b-fe5839925119\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_e159f60f-6686-4ad3-ad6b-fe5839925119 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "            Rank_1    Rank_2   Rank_3    Rank_4   Rank_5   Rank_6    Rank_7  \\\n",
       "coding      coding  learning       is     loves  machine     love     likes   \n",
       "fun            fun      like    likes  learning     love   coding       nlp   \n",
       "i                i     likes      nlp     loves       is     like  learning   \n",
       "is              is      love   coding        me    likes        i     loves   \n",
       "learning  learning    coding        i     loves      fun     like     likes   \n",
       "like          like     likes  machine       fun       me        i      love   \n",
       "likes        likes      like       me     loves        i      fun        is   \n",
       "love          love     loves       is   machine   coding      nlp     likes   \n",
       "loves        loves      love   coding       nlp    likes        i   machine   \n",
       "machine    machine      like     love    coding      nlp    likes     loves   \n",
       "me              me     likes      nlp      like       is  machine      love   \n",
       "nlp            nlp        me    loves      love  machine        i      like   \n",
       "\n",
       "            Rank_8    Rank_9   Rank_10   Rank_11   Rank_12  \n",
       "coding           i      like       fun       nlp        me  \n",
       "fun        machine        me     loves         i        is  \n",
       "i           coding      love   machine        me       fun  \n",
       "is             nlp  learning   machine      like       fun  \n",
       "learning        is      love       nlp   machine        me  \n",
       "like      learning       nlp    coding     loves        is  \n",
       "likes         love   machine  learning    coding       nlp  \n",
       "love          like       fun        me         i  learning  \n",
       "loves     learning        is        me       fun      like  \n",
       "machine         me       fun  learning        is         i  \n",
       "me           loves       fun    coding         i  learning  \n",
       "nlp             is       fun    coding  learning     likes  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Vocabulary (all words in sorted order)\n",
    "vocabulary = sorted(ft_model.model.wv.index_to_key)\n",
    "\n",
    "# Build matrix of word vectors\n",
    "vectors = np.array([ft_model.get_vector(word) for word in vocabulary])\n",
    "\n",
    "# Compute cosine similarity between all words\n",
    "similarity_matrix = cosine_similarity(vectors)\n",
    "\n",
    "# For each word, get words in descending similarity order\n",
    "df_data = [\n",
    "    [vocabulary[idx] for idx in np.argsort(-similarity_matrix[i])]\n",
    "    for i in range(len(vocabulary))\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(df_data, index=vocabulary, columns=[f'Rank_{i+1}' for i in range(len(vocabulary))])\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gGOuZQJQGQTN",
   "metadata": {
    "id": "gGOuZQJQGQTN"
   },
   "source": [
    "# 🧠 Transformer-Based Embeddings\n",
    "\n",
    "In machine learning, data must be **numerically encoded** before it can be used for training or inference.  \n",
    "Encoding transforms symbolic information into numerical form so that mathematical models can process it.\n",
    "\n",
    "- **Categorical Encoding**: Converts discrete categories (e.g., color, species) into numbers.  \n",
    "- **Text Encoding**: Converts text (sentences, documents) into numeric vectors.  \n",
    "- Proper encoding is **critical for model performance**, as it determines how effectively a model can learn underlying patterns.\n",
    "\n",
    "## Evolution of Text Encoding\n",
    "\n",
    "1. **Bag of Words (BoW)** and **TF-IDF**\n",
    "   - Represent text as frequency-based vectors.\n",
    "   - Ignore word order and context.\n",
    "\n",
    "2. **Word2Vec** and **FastText**\n",
    "   - Learn word embeddings from local context using shallow neural networks.\n",
    "   - Capture semantic similarity but are **context-independent** (same embedding for “bank” in *river bank* and *money bank*).\n",
    "\n",
    "3. **Transformer-Based Embeddings**\n",
    "   - Introduced with the **Transformer architecture**, which uses *self-attention* instead of recurrence.\n",
    "   - Generate **contextual embeddings** — word meaning adapts based on sentence context.\n",
    "\n",
    "## Advantages of Transformer-Based Embeddings\n",
    "\n",
    "- **Context Awareness**: Captures meaning depending on surrounding words.  \n",
    "- **Bidirectionality**: Considers both left and right contexts.  \n",
    "- **Long-Range Dependencies**: Understands relationships between distant words.  \n",
    "- **Transfer Learning**: Pre-trained models (like BERT, FNet, RoBERTa) can be fine-tuned for downstream tasks efficiently.\n",
    "\n",
    "### Examples of Transformer Embedding Models\n",
    "\n",
    "| Model       | Core Mechanism       | Key Advantage                    |\n",
    "|------------|--------------------|----------------------------------|\n",
    "| BERT       | Encoder-only       | Deep bidirectional understanding |\n",
    "| RoBERTa    | Optimized BERT     | Better training dynamics          |\n",
    "| DistilBERT | Lightweight BERT   | Faster inference                  |\n",
    "| FNet       | Fourier-based      | Faster and simpler alternative to attention |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9zJ55GjuG8yl",
   "metadata": {
    "id": "9zJ55GjuG8yl"
   },
   "source": [
    "# 🔬 FNetWord — Word-Level Embedding using FNet\n",
    "\n",
    "**FNet** (Fourier Network) is a Transformer-inspired model that replaces the costly *self-attention* mechanism  \n",
    "with a **2D Fourier Transform** applied over the sequence of token embeddings.  \n",
    "This drastically reduces computation while maintaining strong performance on many NLP tasks.\n",
    "\n",
    "## How FNet Works\n",
    "\n",
    "- Input tokens are converted into embeddings.  \n",
    "- Instead of computing pairwise attention weights, FNet applies a **Fourier Transform** to mix information across tokens.  \n",
    "- The resulting transformed representations pass through a **feed-forward network** to produce contextual embeddings.\n",
    "\n",
    "This mechanism captures **global context** efficiently and is computationally much faster than BERT-like attention models.\n",
    "\n",
    "## The `FNetWord` Class\n",
    "\n",
    "- A custom PyTorch module built on top of the pretrained **FNet encoder**.  \n",
    "- Loads a JSON file containing a list of sentences.  \n",
    "- Tokenizes the sentences and extracts contextualized embeddings for **each word**.  \n",
    "- Aggregates subword tokens into a single **word-level vector** using the **mean pooling strategy**.  \n",
    "- Removes all special tokens (e.g., `[CLS]`, `[SEP]`, `[PAD]`), returning only actual words from the JSON file.\n",
    "\n",
    "## Output\n",
    "\n",
    "Returns a Python dictionary:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"word\": embedding_vector\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dA1mCxxdHK31",
   "metadata": {
    "id": "dA1mCxxdHK31"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.optim import AdamW  # Correct import\n",
    "\n",
    "\n",
    "class TransformerEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based word-level embedding extractor using FNet.\n",
    "    Produces contextualized embeddings for each word (aggregated from subwords).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_path: str, model_name: str = \"google/fnet-base\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load input sentences\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.sentences = json.load(f)\n",
    "\n",
    "        if not isinstance(self.sentences, list):\n",
    "            raise ValueError(\"JSON file must contain a list of sentences (list[str]).\")\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def fine_tune(self, lr: float = 2e-5, batch_size: int = 8, epochs: int = 2):\n",
    "        \"\"\"\n",
    "        Simple fine-tuning loop: runs over sentences to update model parameters.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        dataloader = DataLoader(self.sentences[:30], batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(epochs):\n",
    "            for batch in dataloader:\n",
    "                # Tokenize batch\n",
    "                encoded = self.tokenizer(\n",
    "                    batch,\n",
    "                    is_split_into_words=False,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "                )\n",
    "                device = next(self.model.parameters()).device\n",
    "                encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(**encoded)\n",
    "                # Use mean of embeddings as dummy loss (placeholder)\n",
    "                loss = outputs.last_hidden_state.mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    def forward(self, vector_size: int = 768, fine_tune: bool = False) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Compute word embeddings for all words in the loaded sentences.\n",
    "        \"\"\"\n",
    "        if fine_tune:\n",
    "            self.fine_tune(epochs=2)\n",
    "\n",
    "        self.model.train(fine_tune)\n",
    "        word_vectors = defaultdict(list)\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            words = sentence.strip().split()\n",
    "            if not words:\n",
    "                continue\n",
    "\n",
    "            encoded = self.tokenizer(\n",
    "                words,\n",
    "                is_split_into_words=True,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "            # Get word_ids before converting to device\n",
    "            word_ids = encoded.word_ids(batch_index=0)\n",
    "            device = next(self.model.parameters()).device\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "            with torch.set_grad_enabled(fine_tune):\n",
    "                outputs = self.model(**encoded)\n",
    "                last_hidden = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "            token_buckets = defaultdict(list)\n",
    "            for idx, wid in enumerate(word_ids):\n",
    "                if wid is not None:\n",
    "                    token_buckets[wid].append(last_hidden[idx])\n",
    "\n",
    "            for wid, sub_embeds in token_buckets.items():\n",
    "                word = words[wid].lower()\n",
    "                vec = torch.stack(sub_embeds).mean(dim=0)\n",
    "                word_vectors[word].append(vec.detach().cpu())\n",
    "\n",
    "        final_embeddings = {\n",
    "            w: torch.stack(v).mean(dim=0)[:vector_size].tolist()\n",
    "            for w, v in word_vectors.items()\n",
    "            if w not in self.tokenizer.all_special_tokens\n",
    "        }\n",
    "\n",
    "        return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "vLCQQjQ2HOQd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99159,
     "status": "ok",
     "timestamp": 1759733086926,
     "user": {
      "displayName": "SWADHIN DAS",
      "userId": "08011350409236666399"
     },
     "user_tz": -330
    },
    "id": "vLCQQjQ2HOQd",
    "outputId": "867cb25a-9ce5-4b27-ff49-813d69324824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "embedding = TransformerEmbedder(\"Text_Data.json\")\n",
    "embed_vectors = embedding(10,True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
