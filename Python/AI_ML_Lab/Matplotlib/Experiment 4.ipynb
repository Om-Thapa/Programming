{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5476b4-6467-4bbd-9260-1d7d85701fa6",
   "metadata": {},
   "source": [
    "# Introduction to Pandas\n",
    "\n",
    "## What is Pandas\n",
    "\n",
    "- **Pandas** is a powerful open-source Python library for data manipulation and analysis.  \n",
    "- Created by **Wes McKinney** (2008 onward) to provide flexible, high-performance, easy-to-use data structures for working with structured (tabular / time-series) data.\n",
    "\n",
    "## Key Features & Benefits\n",
    "\n",
    "- Works well with **tabular data** (like spreadsheets, SQL tables), where data is organized in rows and columns.  \n",
    "- Supports various file types: **CSV**, **Excel**, **JSON**, **SQL databases** etc.  \n",
    "- Makes cleaning data easier: handling **missing / null values**, detecting and dealing with **duplicates**, converting data types.  \n",
    "- Provides quick and powerful summary and descriptive statistics: minimum, maximum, mean, sum, etc.  \n",
    "- Indexing / alignment: rows/columns are labeled; operations align on labels.  \n",
    "- Integration with **NumPy**, **Matplotlib**, and other data/scientific libraries for numerical computing, visualization, machine learning pipelines.\n",
    "\n",
    "## Main Data Structures\n",
    "\n",
    "| Structure | Description |\n",
    "|-----------|-------------|\n",
    "| **Series** | One-dimensional labeled array. Each element has a label (index). Homogeneous data type (all elements in one Series share a type). |\n",
    "| **DataFrame** | Two-dimensional table of data. Think of it like a spreadsheet: rows and columns. Columns can have different types (numeric, string, datetime). |\n",
    "\n",
    "## Fundamental Concepts & Terminology\n",
    "\n",
    "- **Row** = one observation / record  \n",
    "- **Column** = one feature / attribute / variable  \n",
    "- **Index / Labels** = identifiers for rows (and columns have names)  \n",
    "- **Data types (dtypes)** = what kind of data is stored (integer, float, string/object, datetime, categorical etc)  \n",
    "- **Missing / null** values: often represented by `NaN`, `None`, `NaT`; need special handling  \n",
    "- **Shape** = number of rows & number of columns  \n",
    "- **Size** = total number of elements (rows × columns)  \n",
    "\n",
    "## Example Operations (in principle)\n",
    "\n",
    "Below are typical operations you will do, which we will see in code later:\n",
    "\n",
    "1. **Import / Export**  \n",
    "   - Read a file into a DataFrame (e.g., CSV, Excel)  \n",
    "   - Write a DataFrame back to a file  \n",
    "\n",
    "2. **Inspecting Data**  \n",
    "   - View first few / last few rows  \n",
    "   - Get number of rows & columns  \n",
    "   - Get data types of columns  \n",
    "   - Identify missing values  \n",
    "\n",
    "3. **Descriptive Statistics / Summaries**  \n",
    "   - Count (how many non-missing entries)  \n",
    "   - Sum, Mean (average)  \n",
    "   - Min & Max values  \n",
    "   - Possibly standard deviation, quartiles etc  \n",
    "\n",
    "4. **Cleaning / Preprocessing**  \n",
    "   - Remove or fill missing values  \n",
    "   - Change data types  \n",
    "   - Rename columns  \n",
    "   - Filter / select subsets of data  \n",
    "\n",
    "## Pandas in a Data Workflow\n",
    "\n",
    "1. **Loading / Importing** data from external sources  \n",
    "2. **Exploration / Inspection** to understand the data (shape, types, missingness, basic statistics)  \n",
    "3. **Cleaning / Transforming** data (e.g., handling nulls, converting types, filtering)  \n",
    "4. **Analysis / Aggregation** (grouping, summarization, statistical measures)  \n",
    "5. **Exporting** results or cleaned data for further use or reporting  \n",
    "\n",
    "---\n",
    "\n",
    "*In upcoming session:* We’ll move from this theory to actual code, where you’ll see how to do all these things in Python + Pandas: importing, exploring, summarizing, cleaning, and exporting a dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b2ce8-3d50-48ac-9d59-32b411d01f61",
   "metadata": {},
   "source": [
    "# Installing Pandas\n",
    "\n",
    "Before using Pandas, we need to make sure it is installed in our Python environment.  \n",
    "We can install external Python libraries using **pip** (Python package manager).\n",
    "\n",
    "⚠️ Note: If you are running Jupyter inside Anaconda, Pandas may already be installed.  \n",
    "But to be safe, we will run the installation command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e5dd1d-37f6-4b32-a2ab-7a6f9bc0e30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.0 MB 649.2 kB/s eta 0:00:17\n",
      "   - -------------------------------------- 0.5/11.0 MB 649.2 kB/s eta 0:00:17\n",
      "   - -------------------------------------- 0.5/11.0 MB 649.2 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.8/11.0 MB 567.0 kB/s eta 0:00:18\n",
      "   -- ------------------------------------- 0.8/11.0 MB 567.0 kB/s eta 0:00:18\n",
      "   -- ------------------------------------- 0.8/11.0 MB 567.0 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.0/11.0 MB 475.0 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.0/11.0 MB 475.0 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.0/11.0 MB 475.0 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.0/11.0 MB 475.0 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.0/11.0 MB 475.0 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.0/11.0 MB 475.0 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.0/11.0 MB 475.0 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.0/11.0 MB 475.0 kB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 322.7 kB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 322.7 kB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 322.7 kB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 322.7 kB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 322.7 kB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 322.7 kB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 322.7 kB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 276.6 kB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 276.6 kB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 276.6 kB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 276.6 kB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 276.6 kB/s eta 0:00:35\n",
      "   ------ --------------------------------- 1.8/11.0 MB 268.6 kB/s eta 0:00:35\n",
      "   ------ --------------------------------- 1.8/11.0 MB 268.6 kB/s eta 0:00:35\n",
      "   ------ --------------------------------- 1.8/11.0 MB 268.6 kB/s eta 0:00:35\n",
      "   ------ --------------------------------- 1.8/11.0 MB 268.6 kB/s eta 0:00:35\n",
      "   ------ --------------------------------- 1.8/11.0 MB 268.6 kB/s eta 0:00:35\n",
      "   ------ --------------------------------- 1.8/11.0 MB 268.6 kB/s eta 0:00:35\n",
      "   ------ --------------------------------- 1.8/11.0 MB 268.6 kB/s eta 0:00:35\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 2.1/11.0 MB 249.7 kB/s eta 0:00:36\n",
      "   -------- ------------------------------- 2.4/11.0 MB 217.0 kB/s eta 0:00:40\n",
      "   -------- ------------------------------- 2.4/11.0 MB 217.0 kB/s eta 0:00:40\n",
      "   -------- ------------------------------- 2.4/11.0 MB 217.0 kB/s eta 0:00:40\n",
      "   -------- ------------------------------- 2.4/11.0 MB 217.0 kB/s eta 0:00:40\n",
      "   -------- ------------------------------- 2.4/11.0 MB 217.0 kB/s eta 0:00:40\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.6/11.0 MB 220.3 kB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 200.7 kB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 200.7 kB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 200.7 kB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 200.7 kB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 200.7 kB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 200.7 kB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 200.7 kB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 200.7 kB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 197.0 kB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 197.0 kB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 197.0 kB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 197.0 kB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 197.0 kB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 197.0 kB/s eta 0:00:40\n",
      "   ------------ --------------------------- 3.4/11.0 MB 195.8 kB/s eta 0:00:39\n",
      "   ------------ --------------------------- 3.4/11.0 MB 195.8 kB/s eta 0:00:39\n",
      "   ------------ --------------------------- 3.4/11.0 MB 195.8 kB/s eta 0:00:39\n",
      "   ------------ --------------------------- 3.4/11.0 MB 195.8 kB/s eta 0:00:39\n",
      "   ------------ --------------------------- 3.4/11.0 MB 195.8 kB/s eta 0:00:39\n",
      "   ------------ --------------------------- 3.4/11.0 MB 195.8 kB/s eta 0:00:39\n",
      "   ------------ --------------------------- 3.4/11.0 MB 195.8 kB/s eta 0:00:39\n",
      "   ------------ --------------------------- 3.4/11.0 MB 195.8 kB/s eta 0:00:39\n",
      "   ------------- -------------------------- 3.7/11.0 MB 193.4 kB/s eta 0:00:38\n",
      "   ------------- -------------------------- 3.7/11.0 MB 193.4 kB/s eta 0:00:38\n",
      "   ------------- -------------------------- 3.7/11.0 MB 193.4 kB/s eta 0:00:38\n",
      "   ------------- -------------------------- 3.7/11.0 MB 193.4 kB/s eta 0:00:38\n",
      "   ------------- -------------------------- 3.7/11.0 MB 193.4 kB/s eta 0:00:38\n",
      "   ------------- -------------------------- 3.7/11.0 MB 193.4 kB/s eta 0:00:38\n",
      "   ------------- -------------------------- 3.7/11.0 MB 193.4 kB/s eta 0:00:38\n",
      "   ------------- -------------------------- 3.7/11.0 MB 193.4 kB/s eta 0:00:38\n",
      "   -------------- ------------------------- 3.9/11.0 MB 188.6 kB/s eta 0:00:38\n",
      "   -------------- ------------------------- 3.9/11.0 MB 188.6 kB/s eta 0:00:38\n",
      "   -------------- ------------------------- 3.9/11.0 MB 188.6 kB/s eta 0:00:38\n",
      "   -------------- ------------------------- 3.9/11.0 MB 188.6 kB/s eta 0:00:38\n",
      "   -------------- ------------------------- 3.9/11.0 MB 188.6 kB/s eta 0:00:38\n",
      "   -------------- ------------------------- 3.9/11.0 MB 188.6 kB/s eta 0:00:38\n",
      "   -------------- ------------------------- 3.9/11.0 MB 188.6 kB/s eta 0:00:38\n",
      "   -------------- ------------------------- 3.9/11.0 MB 188.6 kB/s eta 0:00:38\n",
      "   -------------- ------------------------- 3.9/11.0 MB 188.6 kB/s eta 0:00:38\n",
      "   --------------- ------------------------ 4.2/11.0 MB 183.7 kB/s eta 0:00:37\n",
      "   --------------- ------------------------ 4.2/11.0 MB 183.7 kB/s eta 0:00:37\n",
      "   --------------- ------------------------ 4.2/11.0 MB 183.7 kB/s eta 0:00:37\n",
      "   --------------- ------------------------ 4.2/11.0 MB 183.7 kB/s eta 0:00:37\n",
      "   --------------- ------------------------ 4.2/11.0 MB 183.7 kB/s eta 0:00:37\n",
      "   --------------- ------------------------ 4.2/11.0 MB 183.7 kB/s eta 0:00:37\n",
      "   --------------- ------------------------ 4.2/11.0 MB 183.7 kB/s eta 0:00:37\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 184.5 kB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 184.5 kB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 184.5 kB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 184.5 kB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 184.5 kB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 184.5 kB/s eta 0:00:36\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 185.0 kB/s eta 0:00:34\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 185.0 kB/s eta 0:00:34\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 185.0 kB/s eta 0:00:34\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 185.0 kB/s eta 0:00:34\n",
      "   ------------------ --------------------- 5.0/11.0 MB 189.5 kB/s eta 0:00:32\n",
      "   ------------------ --------------------- 5.0/11.0 MB 189.5 kB/s eta 0:00:32\n",
      "   ------------------ --------------------- 5.0/11.0 MB 189.5 kB/s eta 0:00:32\n",
      "   ------------------ --------------------- 5.0/11.0 MB 189.5 kB/s eta 0:00:32\n",
      "   ------------------ --------------------- 5.0/11.0 MB 189.5 kB/s eta 0:00:32\n",
      "   ------------------ --------------------- 5.0/11.0 MB 189.5 kB/s eta 0:00:32\n",
      "   ------------------- -------------------- 5.2/11.0 MB 190.7 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 5.2/11.0 MB 190.7 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 5.2/11.0 MB 190.7 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 5.2/11.0 MB 190.7 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 5.2/11.0 MB 190.7 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 5.2/11.0 MB 190.7 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 5.2/11.0 MB 190.7 kB/s eta 0:00:31\n",
      "   -------------------- ------------------- 5.5/11.0 MB 189.3 kB/s eta 0:00:29\n",
      "   -------------------- ------------------- 5.5/11.0 MB 189.3 kB/s eta 0:00:29\n",
      "   -------------------- ------------------- 5.5/11.0 MB 189.3 kB/s eta 0:00:29\n",
      "   -------------------- ------------------- 5.5/11.0 MB 189.3 kB/s eta 0:00:29\n",
      "   -------------------- ------------------- 5.5/11.0 MB 189.3 kB/s eta 0:00:29\n",
      "   -------------------- ------------------- 5.5/11.0 MB 189.3 kB/s eta 0:00:29\n",
      "   -------------------- ------------------- 5.5/11.0 MB 189.3 kB/s eta 0:00:29\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 5.8/11.0 MB 187.9 kB/s eta 0:00:28\n",
      "   --------------------- ------------------ 6.0/11.0 MB 165.4 kB/s eta 0:00:30\n",
      "   --------------------- ------------------ 6.0/11.0 MB 165.4 kB/s eta 0:00:30\n",
      "   --------------------- ------------------ 6.0/11.0 MB 165.4 kB/s eta 0:00:30\n",
      "   --------------------- ------------------ 6.0/11.0 MB 165.4 kB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 169.0 kB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 169.0 kB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 169.0 kB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 169.0 kB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 6.6/11.0 MB 172.6 kB/s eta 0:00:26\n",
      "   ----------------------- ---------------- 6.6/11.0 MB 172.6 kB/s eta 0:00:26\n",
      "   ----------------------- ---------------- 6.6/11.0 MB 172.6 kB/s eta 0:00:26\n",
      "   ----------------------- ---------------- 6.6/11.0 MB 172.6 kB/s eta 0:00:26\n",
      "   ------------------------ --------------- 6.8/11.0 MB 176.7 kB/s eta 0:00:24\n",
      "   ------------------------ --------------- 6.8/11.0 MB 176.7 kB/s eta 0:00:24\n",
      "   ------------------------ --------------- 6.8/11.0 MB 176.7 kB/s eta 0:00:24\n",
      "   ------------------------ --------------- 6.8/11.0 MB 176.7 kB/s eta 0:00:24\n",
      "   ------------------------ --------------- 6.8/11.0 MB 176.7 kB/s eta 0:00:24\n",
      "   ------------------------ --------------- 6.8/11.0 MB 176.7 kB/s eta 0:00:24\n",
      "   ------------------------ --------------- 6.8/11.0 MB 176.7 kB/s eta 0:00:24\n",
      "   ------------------------- -------------- 7.1/11.0 MB 174.3 kB/s eta 0:00:23\n",
      "   ------------------------- -------------- 7.1/11.0 MB 174.3 kB/s eta 0:00:23\n",
      "   ------------------------- -------------- 7.1/11.0 MB 174.3 kB/s eta 0:00:23\n",
      "   ------------------------- -------------- 7.1/11.0 MB 174.3 kB/s eta 0:00:23\n",
      "   ------------------------- -------------- 7.1/11.0 MB 174.3 kB/s eta 0:00:23\n",
      "   ------------------------- -------------- 7.1/11.0 MB 174.3 kB/s eta 0:00:23\n",
      "   ------------------------- -------------- 7.1/11.0 MB 174.3 kB/s eta 0:00:23\n",
      "   ------------------------- -------------- 7.1/11.0 MB 174.3 kB/s eta 0:00:23\n",
      "   -------------------------- ------------- 7.3/11.0 MB 179.3 kB/s eta 0:00:21\n",
      "   -------------------------- ------------- 7.3/11.0 MB 179.3 kB/s eta 0:00:21\n",
      "   -------------------------- ------------- 7.3/11.0 MB 179.3 kB/s eta 0:00:21\n",
      "   --------------------------- ------------ 7.6/11.0 MB 184.3 kB/s eta 0:00:19\n",
      "   --------------------------- ------------ 7.6/11.0 MB 184.3 kB/s eta 0:00:19\n",
      "   --------------------------- ------------ 7.6/11.0 MB 184.3 kB/s eta 0:00:19\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 188.5 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 188.5 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 188.5 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 188.5 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 188.5 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 188.5 kB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 187.9 kB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 187.9 kB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 187.9 kB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 187.9 kB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 187.9 kB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 187.9 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 8.4/11.0 MB 193.9 kB/s eta 0:00:14\n",
      "   ------------------------------ --------- 8.4/11.0 MB 193.9 kB/s eta 0:00:14\n",
      "   ------------------------------ --------- 8.4/11.0 MB 193.9 kB/s eta 0:00:14\n",
      "   ------------------------------ --------- 8.4/11.0 MB 193.9 kB/s eta 0:00:14\n",
      "   ------------------------------ --------- 8.4/11.0 MB 193.9 kB/s eta 0:00:14\n",
      "   ------------------------------ --------- 8.4/11.0 MB 193.9 kB/s eta 0:00:14\n",
      "   ------------------------------ --------- 8.4/11.0 MB 193.9 kB/s eta 0:00:14\n",
      "   ------------------------------ --------- 8.4/11.0 MB 193.9 kB/s eta 0:00:14\n",
      "   ------------------------------- -------- 8.7/11.0 MB 192.8 kB/s eta 0:00:13\n",
      "   ------------------------------- -------- 8.7/11.0 MB 192.8 kB/s eta 0:00:13\n",
      "   ------------------------------- -------- 8.7/11.0 MB 192.8 kB/s eta 0:00:13\n",
      "   ------------------------------- -------- 8.7/11.0 MB 192.8 kB/s eta 0:00:13\n",
      "   ------------------------------- -------- 8.7/11.0 MB 192.8 kB/s eta 0:00:13\n",
      "   -------------------------------- ------- 8.9/11.0 MB 194.9 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 8.9/11.0 MB 194.9 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 8.9/11.0 MB 194.9 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 8.9/11.0 MB 194.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 9.2/11.0 MB 199.1 kB/s eta 0:00:10\n",
      "   --------------------------------- ------ 9.2/11.0 MB 199.1 kB/s eta 0:00:10\n",
      "   --------------------------------- ------ 9.2/11.0 MB 199.1 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 9.4/11.0 MB 204.4 kB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 9.4/11.0 MB 204.4 kB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 210.1 kB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 210.1 kB/s eta 0:00:07\n",
      "   ------------------------------------ --- 10.0/11.0 MB 218.8 kB/s eta 0:00:05\n",
      "   ------------------------------------- -- 10.2/11.0 MB 225.6 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 10.2/11.0 MB 225.6 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 10.5/11.0 MB 231.0 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 10.5/11.0 MB 231.0 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 10.5/11.0 MB 231.0 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.7/11.0 MB 240.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 240.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 240.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 244.1 kB/s  0:00:48\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "Successfully installed pandas-2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Install pandas using pip\n",
    "# The \"!\" allows us to run shell commands directly from Jupyter Notebook.\n",
    "!pip install -U pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d45aaf-5224-402d-84f9-37265a8d04b9",
   "metadata": {},
   "source": [
    "# Importing Pandas\n",
    "\n",
    "Once Pandas is installed, we need to import it into our notebook.  \n",
    "By convention, Pandas is almost always imported with the alias **pd**.  \n",
    "This makes it easier and shorter to call its functions and classes.\n",
    "\n",
    "For example:  \n",
    "- Instead of writing `pandas.DataFrame()`,  \n",
    "- We can simply write `pd.DataFrame()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa8581e-9aad-4fee-8dd1-675e4e6ee7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Importing pandas using the standard alias 'pd'\n",
    "import pandas as pd\n",
    "\n",
    "# Checking version to confirm import worked\n",
    "print(\"Pandas version:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b156368-5995-4c22-8262-2d14fc9fbb63",
   "metadata": {},
   "source": [
    "# Creating a DataFrame from a List\n",
    "\n",
    "We can create a DataFrame using a **list of lists** (rows of data).  \n",
    "While doing this, we must also specify the **column names** explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5950827-1c28-4e3e-92ee-4769e100ae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Name  Age\n",
      "0   1    Alice   23\n",
      "1   2      Bob   27\n",
      "2   3  Charlie   22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame from a list of lists\n",
    "data_list = [\n",
    "    [1, \"Alice\", 23],\n",
    "    [2, \"Bob\", 27],\n",
    "    [3, \"Charlie\", 22]\n",
    "]\n",
    "columns = [\"ID\", \"Name\", \"Age\"]\n",
    "df_from_list = pd.DataFrame(data_list, columns=columns)\n",
    "\n",
    "print(df_from_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e621e-98c1-4a98-ab23-5d34876ac3d4",
   "metadata": {},
   "source": [
    "# Creating a DataFrame from a Dictionary\n",
    "\n",
    "Another common way is to use a **Python dictionary**.  \n",
    "- Keys of the dictionary become the **column names**.  \n",
    "- Values (lists) become the **column data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557b7dcc-5869-4f2c-b0dd-cae10db4edf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Name  Age\n",
      "0   1    Alice   23\n",
      "1   2      Bob   27\n",
      "2   3  Charlie   22\n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame from a dictionary\n",
    "data_dict = {\n",
    "    \"ID\": [1, 2, 3],\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [23, 27, 22]\n",
    "}\n",
    "\n",
    "df_from_dict = pd.DataFrame(data_dict)\n",
    "\n",
    "print(df_from_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc577ac5-31ad-43f6-a5cf-43a7148ab185",
   "metadata": {},
   "source": [
    "# Creating a DataFrame from a Dictionary with Custom Index\n",
    "\n",
    "Pandas allows us to specify custom **row labels (index)** while creating a DataFrame.  \n",
    "For example, instead of default numeric indices (0, 1, 2),  \n",
    "we can use identifiers like **emp001, emp002, emp003**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128d2dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emp000', 'emp001', 'emp002', 'emp003', 'emp004', 'emp005', 'emp006', 'emp007', 'emp008', 'emp009']\n"
     ]
    }
   ],
   "source": [
    "custom_index = [f\"emp{i:03d}\" for i in range(10)];\n",
    "print(custom_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9253165-1ffa-4fd6-9d92-6251360f5615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Custom Index:\n",
      "\n",
      "        ID     Name  Age\n",
      "emp001   1    Alice   23\n",
      "emp002   2      Bob   27\n",
      "emp003   3  Charlie   22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dictionary with column data\n",
    "data_dict = {\n",
    "    \"ID\": [1, 2, 3],\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [23, 27, 22]\n",
    "}\n",
    "\n",
    "# Custom index names\n",
    "custom_index = [\"emp001\", \"emp002\", \"emp003\"]\n",
    "\n",
    "# Creating DataFrame with custom index\n",
    "df_custom_index = pd.DataFrame(data_dict, index=custom_index)\n",
    "\n",
    "print(\"DataFrame with Custom Index:\\n\")\n",
    "print(df_custom_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd2dab-0849-4dee-96eb-610c7f2c0310",
   "metadata": {},
   "source": [
    "# A Unified Function to Convert Data into a Pandas DataFrame\n",
    "\n",
    "We will write a function `convert_to_pd(data, indexes=None, send_column_info=True)`  \n",
    "that takes either:\n",
    "1. A **list of lists** → where the first inner list contains column names, and the rest are row values.  \n",
    "   - In this case, `send_column_info` **must be set to True**.  \n",
    "   - All rows must have equal length.  \n",
    "2. A **dictionary** → where keys are column names and values are lists.  \n",
    "   - All lists must be of **equal length**.  \n",
    "\n",
    "If the input is invalid, an error will be raised.  \n",
    "Otherwise, the function will return a valid **Pandas DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e625b957-b209-42e0-9896-c67a406a2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def _validate_data(data, send_column_info):\n",
    "    \"\"\"Helper function to validate the input before creating DataFrame.\"\"\"\n",
    "    # Case 1: List of lists\n",
    "    if isinstance(data, list):\n",
    "        if not all(isinstance(row, list) for row in data):\n",
    "            raise TypeError(\"All elements must be lists when passing a list of lists.\")\n",
    "        if not send_column_info:\n",
    "            raise ValueError(\"send_column_info must be True when passing a list of lists.\")\n",
    "        # check equal length\n",
    "        length = len(data[0])\n",
    "        if not all(len(row) == length for row in data):\n",
    "            raise ValueError(\"All rows (including column info) must have the same length.\")\n",
    "        return \"list\"\n",
    "\n",
    "    # Case 2: Dictionary\n",
    "    elif isinstance(data, dict):\n",
    "        lengths = [len(v) for v in data.values()]\n",
    "        if len(set(lengths)) != 1:\n",
    "            raise ValueError(\"All columns in the dictionary must have equal length.\")\n",
    "        return \"dict\"\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Data must be either a list of lists or a dictionary.\")\n",
    "\n",
    "\n",
    "def convert_to_pd(data, indexes=None, send_column_info=True):\n",
    "    \"\"\"\n",
    "        Convert structured data into a Pandas DataFrame.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : list of lists or dict\n",
    "            If a list of lists is provided:\n",
    "            - The first inner list must contain column names.\n",
    "            - The following inner lists contain row data.\n",
    "            - In this case, `send_column_info` must be set to True.\n",
    "            \n",
    "            If a dictionary is provided:\n",
    "            - Keys become column names.\n",
    "            - Values must be lists of equal length (each list represents column data).\n",
    "    \n",
    "        indexes : list, optional\n",
    "            Custom index labels for the DataFrame rows. \n",
    "            If None, default numeric indices are used.\n",
    "    \n",
    "        send_column_info : bool, default=True\n",
    "            Used only when data is a list of lists.\n",
    "            Ensures that the first item in the list contains column names.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A pandas DataFrame created from the given data.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        TypeError, ValueError\n",
    "            If the input data is invalid.\n",
    "    \"\"\"\n",
    "    dtype = _validate_data(data, send_column_info)\n",
    "\n",
    "    if dtype == \"list\":\n",
    "        columns = data[0]\n",
    "        rows = data[1:]\n",
    "        return pd.DataFrame(rows, columns=columns, index=indexes)\n",
    "    \n",
    "    elif dtype == \"dict\":\n",
    "        return pd.DataFrame(data, index=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2afa710b-8473-4abf-b1ff-4545ec65d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Name  Age\n",
      "0   1    Alice   23\n",
      "1   2      Bob   27\n",
      "2   3  Charlie   22\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Environment 1:\n",
    "Creating a DataFrame from a list of lists.\n",
    "The first row contains column names, and the rest are data rows.\n",
    "\"\"\"\n",
    "\n",
    "data_list = [\n",
    "    [\"ID\", \"Name\", \"Age\"],\n",
    "    [1, \"Alice\", 23],\n",
    "    [2, \"Bob\", 27],\n",
    "    [3, \"Charlie\", 22]\n",
    "]\n",
    "\n",
    "df1 = convert_to_pd(data_list)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a89125-ab54-4af4-a1db-13b04b768f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Name  Age\n",
      "0   1    Alice   23\n",
      "1   2      Bob   27\n",
      "2   3  Charlie   22\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Environment 2:\n",
    "Creating a DataFrame from a dictionary.\n",
    "Keys become column names and values are lists of equal length.\n",
    "\"\"\"\n",
    "\n",
    "data_dict = {\n",
    "    \"ID\": [1, 2, 3],\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [23, 27, 22]\n",
    "}\n",
    "\n",
    "df2 = convert_to_pd(data_dict)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c96dfcc-3dc2-495c-9080-c738cdbb404b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID     Name  Age\n",
      "emp001   1    Alice   23\n",
      "emp002   2      Bob   27\n",
      "emp003   3  Charlie   22\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Environment 3:\n",
    "Creating a DataFrame from a list of lists with custom index labels.\n",
    "The first row contains column names and custom row labels are provided.\n",
    "\"\"\"\n",
    "\n",
    "data_list = [\n",
    "    [\"ID\", \"Name\", \"Age\"],\n",
    "    [1, \"Alice\", 23],\n",
    "    [2, \"Bob\", 27],\n",
    "    [3, \"Charlie\", 22]\n",
    "]\n",
    "\n",
    "custom_index = [\"emp001\", \"emp002\", \"emp003\"]\n",
    "\n",
    "df3 = convert_to_pd(data_list, indexes=custom_index)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee29f5cd-ee78-4896-8845-221cf5d6fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID     Name  Age\n",
      "emp001   1    Alice   23\n",
      "emp002   2      Bob   27\n",
      "emp003   3  Charlie   22\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Environment 4:\n",
    "Creating a DataFrame from a dictionary with custom index labels.\n",
    "Each key-value pair becomes a column with provided row labels.\n",
    "\"\"\"\n",
    "data_dict = {\n",
    "    \"ID\": [1, 2, 3],\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [23, 27, 22]\n",
    "}\n",
    "\n",
    "custom_index = [\"emp001\", \"emp002\", \"emp003\"]\n",
    "\n",
    "df4 = convert_to_pd(data_dict, indexes=custom_index)\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "183f767f-597c-408b-8441-bf69f47f50a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows (including column info) must have the same length.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Environment 5:\n",
    "Attempting to create a DataFrame from a list of lists with unequal row lengths.\n",
    "This should raise a ValueError because row sizes are inconsistent.\n",
    "\"\"\"\n",
    "\n",
    "bad_data_list = [\n",
    "    [\"ID\", \"Name\", \"Age\"],\n",
    "    [1, \"Alice\", 23],\n",
    "    [2, \"Bob\"]   # Missing one value\n",
    "]\n",
    "\n",
    "try:\n",
    "    df5 = convert_to_pd(bad_data_list)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7cd8349-90e3-4250-adeb-5dd9b65e4cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns in the dictionary must have equal length.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Environment 6:\n",
    "Attempting to create a DataFrame from a dictionary with unequal list lengths.\n",
    "This should raise a ValueError because all columns must have the same number of rows.\n",
    "\"\"\"\n",
    "\n",
    "bad_data_dict = {\n",
    "    \"ID\": [1, 2, 3],\n",
    "    \"Name\": [\"Alice\", \"Bob\"],  # Fewer values\n",
    "    \"Age\": [23, 27, 22]\n",
    "}\n",
    "\n",
    "try:\n",
    "    df6 = convert_to_pd(bad_data_dict)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1e9214-a1ac-45c4-b203-6650fdbd0252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send_column_info must be True when passing a list of lists.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Environment 7:\n",
    "Passing a list of lists but setting send_column_info=False (default overridden).\n",
    "This should raise a ValueError because column information is set to send_column_info=False.\n",
    "\"\"\"\n",
    "false_param_data_list = [\n",
    "    [\"ID\", \"Name\", \"Age\"],\n",
    "    [1, \"Alice\", 23],\n",
    "    [2, \"Bob\", 27]\n",
    "]\n",
    "\n",
    "try:\n",
    "    df7 = convert_to_pd(false_param_data_list,send_column_info=False)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9cc86-d9e8-4fe4-a56a-7a711bc36542",
   "metadata": {},
   "source": [
    "# Importing a Popular Dataset with Pandas\n",
    "\n",
    "We often use Pandas to load datasets from files such as CSV, Excel, or online sources.  \n",
    "Here we will import the **Iris dataset**, which is a very popular dataset in machine learning.  \n",
    "It contains measurements of flower species (setosa, versicolor, virginica) with features like petal and sepal length/width.\n",
    "\n",
    "We will read it directly from an online CSV file using `pd.read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfca3e0d-c856-4b46-a33e-e2c062c29eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris dataset:\n",
      "\n",
      "     rowid    species     island  bill_length_mm  bill_depth_mm  \\\n",
      "0        1     Adelie  Torgersen            39.1           18.7   \n",
      "1        2     Adelie  Torgersen            39.5           17.4   \n",
      "2        3     Adelie  Torgersen            40.3           18.0   \n",
      "3        4     Adelie  Torgersen             NaN            NaN   \n",
      "4        5     Adelie  Torgersen            36.7           19.3   \n",
      "..     ...        ...        ...             ...            ...   \n",
      "339    340  Chinstrap      Dream            55.8           19.8   \n",
      "340    341  Chinstrap      Dream            43.5           18.1   \n",
      "341    342  Chinstrap      Dream            49.6           18.2   \n",
      "342    343  Chinstrap      Dream            50.8           19.0   \n",
      "343    344  Chinstrap      Dream            50.2           18.7   \n",
      "\n",
      "     flipper_length_mm  body_mass_g     sex  year  \n",
      "0                181.0       3750.0    male  2007  \n",
      "1                186.0       3800.0  female  2007  \n",
      "2                195.0       3250.0  female  2007  \n",
      "3                  NaN          NaN     NaN  2007  \n",
      "4                193.0       3450.0  female  2007  \n",
      "..                 ...          ...     ...   ...  \n",
      "339              207.0       4000.0    male  2009  \n",
      "340              202.0       3400.0  female  2009  \n",
      "341              193.0       3775.0    male  2009  \n",
      "342              210.0       4100.0    male  2009  \n",
      "343              198.0       3775.0  female  2009  \n",
      "\n",
      "[344 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing the Iris dataset from UCI repository\n",
    "# url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n",
    "url = \"https://quorumlanguage.com/files/blocks/penguins.csv\"\n",
    "\n",
    "iris_df = pd.read_csv(url)\n",
    "\n",
    "print(\"Iris dataset:\\n\")\n",
    "print(iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e968e-438d-4387-8525-b79ae22064f2",
   "metadata": {},
   "source": [
    "# Number of Rows\n",
    "\n",
    "The number of rows tells us how many observations (records) are present in the dataset.  \n",
    "We can get this by checking the shape of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19b4d5ad-4485-45a8-bb4a-c4b5014b071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 344\n"
     ]
    }
   ],
   "source": [
    "# Number of rows in the dataset\n",
    "num_rows = iris_df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0e94d-7152-4f9d-a736-05fae1c2c846",
   "metadata": {},
   "source": [
    "# Number of Columns\n",
    "\n",
    "The number of columns tells us how many features (variables) are present in the dataset.  \n",
    "We can also check this using the shape of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b98c2081-a96f-4df1-9920-bbc3d475b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 9\n"
     ]
    }
   ],
   "source": [
    "# Number of columns in the dataset\n",
    "num_columns = iris_df.shape[1]\n",
    "print(\"Number of columns:\", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4031cd66-a23c-4e0e-bc2c-fb4689a3741c",
   "metadata": {},
   "source": [
    "# First Five Rows\n",
    "\n",
    "We use `.head()` to display the first five rows.  \n",
    "This gives us a quick look at the structure and content of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25417563-f922-4398-a6ab-ba91c9af6d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows:\n",
      "\n",
      "   rowid species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0      1  Adelie  Torgersen            39.1           18.7              181.0   \n",
      "1      2  Adelie  Torgersen            39.5           17.4              186.0   \n",
      "2      3  Adelie  Torgersen            40.3           18.0              195.0   \n",
      "3      4  Adelie  Torgersen             NaN            NaN                NaN   \n",
      "4      5  Adelie  Torgersen            36.7           19.3              193.0   \n",
      "\n",
      "   body_mass_g     sex  year  \n",
      "0       3750.0    male  2007  \n",
      "1       3800.0  female  2007  \n",
      "2       3250.0  female  2007  \n",
      "3          NaN     NaN  2007  \n",
      "4       3450.0  female  2007  \n"
     ]
    }
   ],
   "source": [
    "# Display first five rows of the dataset\n",
    "print(\"First five rows:\\n\")\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36479d2d-eaa6-4359-9198-55a1ff60d297",
   "metadata": {},
   "source": [
    "# Last Five Rows\n",
    "\n",
    "We use `.tail()` to display the last five rows of the dataset.  \n",
    "This helps us see the ending portion of the data and confirm if rows are ordered properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ba5d896-a400-4c21-a61f-721c5a6eb71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last five rows:\n",
      "\n",
      "     sepal_length  sepal_width  petal_length  petal_width    species\n",
      "145           6.7          3.0           5.2          2.3  virginica\n",
      "146           6.3          2.5           5.0          1.9  virginica\n",
      "147           6.5          3.0           5.2          2.0  virginica\n",
      "148           6.2          3.4           5.4          2.3  virginica\n",
      "149           5.9          3.0           5.1          1.8  virginica\n"
     ]
    }
   ],
   "source": [
    "# Display last five rows of the dataset\n",
    "print(\"Last five rows:\\n\")\n",
    "print(iris_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbb4b4-1c52-4db5-b89f-6e6308954a95",
   "metadata": {},
   "source": [
    "# Size of the Dataset\n",
    "\n",
    "The size gives the total number of elements in the dataset.  \n",
    "This is equal to (rows × columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdc19dc0-04f0-49f1-8c15-41d860ddedee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 750\n"
     ]
    }
   ],
   "source": [
    "# Size of the dataset (total elements)\n",
    "print(\"Size of dataset:\", iris_df.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c08c9c-2532-49f7-b78d-3baf941cdb32",
   "metadata": {},
   "source": [
    "# Number of Missing Values\n",
    "\n",
    "Missing values can affect analysis.  \n",
    "We use `.isnull().sum()` to check how many missing values are in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab5e29d0-542b-42a9-8aad-f95b814965bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "\n",
      "rowid                 0\n",
      "species               0\n",
      "island                0\n",
      "bill_length_mm        2\n",
      "bill_depth_mm         2\n",
      "flipper_length_mm     2\n",
      "body_mass_g           2\n",
      "sex                  11\n",
      "year                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count of missing values per column\n",
    "print(\"Missing values in each column:\\n\")\n",
    "print(iris_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ded874-de63-4adb-bdae-421785f12cac",
   "metadata": {},
   "source": [
    "# Sum of Numerical Columns\n",
    "\n",
    "We use `.sum()` on the DataFrame to compute column-wise sums.  \n",
    "This will only apply to numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4c772f2-468e-4a1c-a357-b5f15f5c5747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of numerical columns:\n",
      "\n",
      "rowid                  59340.0\n",
      "bill_length_mm         15021.3\n",
      "bill_depth_mm           5865.7\n",
      "flipper_length_mm      68713.0\n",
      "body_mass_g          1437000.0\n",
      "year                  690762.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Sum of all numerical columns\n",
    "print(\"Sum of numerical columns:\\n\")\n",
    "print(iris_df.sum(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd92a8a-c89d-4b18-953d-eac25051eb11",
   "metadata": {},
   "source": [
    "# Average (Mean) of Numerical Columns\n",
    "\n",
    "The average gives the central tendency of each numerical column.  \n",
    "We use `.mean()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76e588f8-214b-44f2-9da7-eea68c2dd2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of numerical columns:\n",
      "\n",
      "rowid                 172.500000\n",
      "bill_length_mm         43.921930\n",
      "bill_depth_mm          17.151170\n",
      "flipper_length_mm     200.915205\n",
      "body_mass_g          4201.754386\n",
      "year                 2008.029070\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Average (mean) of all numerical columns\n",
    "print(\"Average of numerical columns:\\n\")\n",
    "print(iris_df.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb2788-753a-4e04-a5cf-d422d053bf8e",
   "metadata": {},
   "source": [
    "# Minimum Values of Numerical Columns\n",
    "\n",
    "The minimum value indicates the smallest entry in each numerical column.  \n",
    "We use `.min()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29f41e3f-2259-4c8d-9dd5-d30ab6b4b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum values of numerical columns:\n",
      "\n",
      "rowid                   1.0\n",
      "bill_length_mm         32.1\n",
      "bill_depth_mm          13.1\n",
      "flipper_length_mm     172.0\n",
      "body_mass_g          2700.0\n",
      "year                 2007.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Minimum values of all numerical columns\n",
    "print(\"Minimum values of numerical columns:\\n\")\n",
    "print(iris_df.min(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734439e7-5bcf-4fa1-9bfd-a5f56b7fd51b",
   "metadata": {},
   "source": [
    "# Maximum Values of Numerical Columns\n",
    "\n",
    "The maximum value indicates the largest entry in each numerical column.  \n",
    "We use `.max()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d302e0cc-88b8-41ae-bf39-280453e5ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum values of numerical columns:\n",
      "\n",
      "sepal_length    7.9\n",
      "sepal_width     4.4\n",
      "petal_length    6.9\n",
      "petal_width     2.5\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Maximum values of all numerical columns\n",
    "print(\"Maximum values of numerical columns:\\n\")\n",
    "print(iris_df.max(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576382d-c927-49d3-a231-d6caff9e4c81",
   "metadata": {},
   "source": [
    "# Dataset Summary Function\n",
    "\n",
    "We now define a single function `dataset_summary()` that takes either:\n",
    "- A **URL / file path** to a CSV file  \n",
    "- Or a **Pandas DataFrame** directly  \n",
    "\n",
    "It will print all the dataset details in order:  \n",
    "1. Number of rows  \n",
    "2. Number of columns  \n",
    "3. First 5 rows  \n",
    "4. Last 5 rows  \n",
    "5. Size  \n",
    "6. Missing values  \n",
    "7. Sum of numerical columns  \n",
    "8. Average of numerical columns  \n",
    "9. Minimum values  \n",
    "10. Maximum values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42e28258-6866-4e69-bdc1-714b2774f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dataset_summary(data):\n",
    "    \"\"\"\n",
    "        Display a detailed summary of a dataset.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : str or pd.DataFrame\n",
    "            - If str: assumed to be a URL or file path to a CSV file.\n",
    "            - If DataFrame: used directly.\n",
    "    \n",
    "        Prints\n",
    "        ------\n",
    "        - Number of rows\n",
    "        - Number of columns\n",
    "        - First 5 rows\n",
    "        - Last 5 rows\n",
    "        - Size (total elements)\n",
    "        - Missing values per column\n",
    "        - Sum of numerical columns\n",
    "        - Average (mean) of numerical columns\n",
    "        - Minimum values of numerical columns\n",
    "        - Maximum values of numerical columns\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_csv(data)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        df = data\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a URL/file path (str) or a Pandas DataFrame.\")\n",
    "\n",
    "    # 1. Number of rows\n",
    "    print(\"\\033[1mNumber of rows:\\033[0m\", df.shape[0], \"\\n\")\n",
    "\n",
    "    # 2. Number of columns\n",
    "    print(\"\\033[1mNumber of columns:\\033[0m\", df.shape[1], \"\\n\")\n",
    "\n",
    "    # 3. First 5 rows\n",
    "    print(\"\\033[1mFirst 5 rows:\\n\\033[0m\", df.head(), \"\\n\")\n",
    "\n",
    "    # 4. Last 5 rows\n",
    "    print(\"\\033[1mLast 5 rows:\\n\\033[0m\", df.tail(), \"\\n\")\n",
    "\n",
    "    # 5. Size\n",
    "    print(\"\\033[1mSize of dataset (total elements): \\033[0m\", df.size, \"\\n\")\n",
    "\n",
    "    # 6. Missing values\n",
    "    print(\"\\033[1mMissing values per column:\\n\\033[0m\", df.isnull().sum(), \"\\n\")\n",
    "\n",
    "    # 7. Sum of numerical columns\n",
    "    print(\"\\033[1mSum of numerical columns:\\n\\033[0m\", df.sum(numeric_only=True), \"\\n\")\n",
    "\n",
    "    # 8. Average of numerical columns\n",
    "    print(\"\\033[1mAverage of numerical columns:\\n\\033[0m\", df.mean(numeric_only=True), \"\\n\")\n",
    "\n",
    "    # 9. Minimum values\n",
    "    print(\"\\033[1mMinimum values of numerical columns:\\n\\033[0m\", df.min(numeric_only=True), \"\\n\")\n",
    "\n",
    "    # 10. Maximum values\n",
    "    print(\"\\033[1mMaximum values of numerical columns:\\n\\033[0m\", df.max(numeric_only=True), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ec2d056-ba8d-400b-90f2-27d9f8d7efa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNumber of rows:\u001b[0m 150 \n",
      "\n",
      "\u001b[1mNumber of columns:\u001b[0m 5 \n",
      "\n",
      "\u001b[1mFirst 5 rows:\n",
      "\u001b[0m    sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa \n",
      "\n",
      "\u001b[1mLast 5 rows:\n",
      "\u001b[0m      sepal_length  sepal_width  petal_length  petal_width    species\n",
      "145           6.7          3.0           5.2          2.3  virginica\n",
      "146           6.3          2.5           5.0          1.9  virginica\n",
      "147           6.5          3.0           5.2          2.0  virginica\n",
      "148           6.2          3.4           5.4          2.3  virginica\n",
      "149           5.9          3.0           5.1          1.8  virginica \n",
      "\n",
      "\u001b[1mSize of dataset (total elements): \u001b[0m 750 \n",
      "\n",
      "\u001b[1mMissing values per column:\n",
      "\u001b[0m sepal_length    0\n",
      "sepal_width     0\n",
      "petal_length    0\n",
      "petal_width     0\n",
      "species         0\n",
      "dtype: int64 \n",
      "\n",
      "\u001b[1mSum of numerical columns:\n",
      "\u001b[0m sepal_length    876.5\n",
      "sepal_width     458.6\n",
      "petal_length    563.7\n",
      "petal_width     179.9\n",
      "dtype: float64 \n",
      "\n",
      "\u001b[1mAverage of numerical columns:\n",
      "\u001b[0m sepal_length    5.843333\n",
      "sepal_width     3.057333\n",
      "petal_length    3.758000\n",
      "petal_width     1.199333\n",
      "dtype: float64 \n",
      "\n",
      "\u001b[1mMinimum values of numerical columns:\n",
      "\u001b[0m sepal_length    4.3\n",
      "sepal_width     2.0\n",
      "petal_length    1.0\n",
      "petal_width     0.1\n",
      "dtype: float64 \n",
      "\n",
      "\u001b[1mMaximum values of numerical columns:\n",
      "\u001b[0m sepal_length    7.9\n",
      "sepal_width     4.4\n",
      "petal_length    6.9\n",
      "petal_width     2.5\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage with Iris dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n",
    "dataset_summary(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf016a-a5b6-4cbd-b6d0-3aa10f2cb996",
   "metadata": {},
   "source": [
    "# Creating a DataFrame and Demonstrating loc vs iloc\n",
    "\n",
    "We will create a DataFrame using the `convert_to_pd` helper function we defined earlier,  \n",
    "with a dictionary as input.\n",
    "\n",
    "Then we will demonstrate the difference between **`.loc`** and **`.iloc`**:\n",
    "\n",
    "- `.loc` → selects rows and columns **by labels** (index names or column names)  \n",
    "- `.iloc` → selects rows and columns **by integer positions** (0-based indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7f7aaa9-8a25-4ddf-8afc-52939c01c83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataFrame:\u001b[0m\n",
      "         ID     Name  Age\n",
      "emp001  101    Alice   23\n",
      "emp002  102      Bob   27\n",
      "emp003  103  Charlie   22\n",
      "\n",
      "\u001b[1mUsing .loc to select row 'emp002' and column 'Name':\u001b[0m Bob\n",
      "\n",
      "\u001b[1mUsing .loc to select multiple rows and columns:\u001b[0m\n",
      "         ID  Age\n",
      "emp001  101   23\n",
      "emp003  103   22\n",
      "\n",
      "\u001b[1mUsing .iloc to select the second row and third column (position based):\u001b[0m 27\n",
      "\n",
      "\u001b[1mUsing .iloc to select multiple rows and columns (positions):\u001b[0m\n",
      "         ID  Age\n",
      "emp001  101   23\n",
      "emp003  103   22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary input\n",
    "data_dict = {\n",
    "    \"ID\": [101, 102, 103],\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [23, 27, 22]\n",
    "}\n",
    "\n",
    "# Custom index\n",
    "custom_index = [\"emp001\", \"emp002\", \"emp003\"]\n",
    "\n",
    "# Create DataFrame using helper function\n",
    "df_demo = convert_to_pd(data_dict, indexes=custom_index)\n",
    "\n",
    "# Bold header\n",
    "bold = \"\\033[1m\"\n",
    "reset = \"\\033[0m\"\n",
    "\n",
    "print(f\"{bold}DataFrame:{reset}\")\n",
    "print(df_demo, end=\"\\n\\n\")\n",
    "\n",
    "# --- Demonstrate loc ---\n",
    "print(f\"{bold}Using .loc to select row 'emp002' and column 'Name':{reset}\", df_demo.loc[\"emp002\", \"Name\"], end=\"\\n\\n\")\n",
    "\n",
    "print(f\"{bold}Using .loc to select multiple rows and columns:{reset}\")\n",
    "print(df_demo.loc[[\"emp001\", \"emp003\"], [\"ID\", \"Age\"]], end=\"\\n\\n\")\n",
    "\n",
    "# --- Demonstrate iloc ---\n",
    "print(f\"{bold}Using .iloc to select the second row and third column (position based):{reset}\", df_demo.iloc[1, 2], end=\"\\n\\n\")\n",
    "\n",
    "print(f\"{bold}Using .iloc to select multiple rows and columns (positions):{reset}\")\n",
    "print(df_demo.iloc[[0, 2], [0, 2]], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cdd086-f410-4597-805b-64d0f4d010e0",
   "metadata": {},
   "source": [
    "# Creating Base DataFrame\n",
    "\n",
    "We will create a DataFrame from a dictionary that will be used for all upcoming operations.  \n",
    "This DataFrame has 5 rows, multiple numerical and categorical columns, and custom indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c857f87c-8344-4e80-b42f-8dc1010a8377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBase DataFrame:\u001b[0m\n",
      "         ID     Name  Age  Salary Department\n",
      "emp001  101    Alice   23   50000         HR\n",
      "emp002  102      Bob   27   60000         IT\n",
      "emp003  103  Charlie   22   55000         IT\n",
      "emp004  104    David   25   65000    Finance\n",
      "emp005  105      Eva   30   70000         HR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample dictionary\n",
    "data_dict = {\n",
    "    \"ID\": [101, 102, 103, 104, 105],\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"],\n",
    "    \"Age\": [23, 27, 22, 25, 30],\n",
    "    \"Salary\": [50000, 60000, 55000, 65000, 70000],\n",
    "    \"Department\": [\"HR\", \"IT\", \"IT\", \"Finance\", \"HR\"]\n",
    "}\n",
    "\n",
    "# Custom index\n",
    "custom_index = [\"emp001\", \"emp002\", \"emp003\", \"emp004\", \"emp005\"]\n",
    "\n",
    "# Create DataFrame using helper function\n",
    "df = convert_to_pd(data_dict, indexes=custom_index)\n",
    "\n",
    "bold = \"\\033[1m\"\n",
    "reset = \"\\033[0m\"\n",
    "\n",
    "print(f\"{bold}Base DataFrame:{reset}\")\n",
    "print(df, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c73fe6-6131-4682-9b20-bc8abc57b295",
   "metadata": {},
   "source": [
    "# Slicing & Boolean Indexing\n",
    "\n",
    "We will demonstrate:\n",
    "- Selecting rows using integer positions (`.iloc`)  \n",
    "- Selecting rows/columns by labels (`.loc`)  \n",
    "- Conditional selection based on numerical or categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c5b288a-6a42-412d-b3b2-640d5baa0c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFirst 3 rows using iloc:\u001b[0m\n",
      "         ID     Name  Age  Salary Department\n",
      "emp001  101    Alice   23   50000         HR\n",
      "emp002  102      Bob   27   60000         IT\n",
      "emp003  103  Charlie   22   55000         IT\n",
      "\n",
      "\u001b[1mSalary column using loc:\u001b[0m\n",
      "emp001    50000\n",
      "emp002    60000\n",
      "emp003    55000\n",
      "emp004    65000\n",
      "emp005    70000\n",
      "Name: Salary, dtype: int64\n",
      "\n",
      "\u001b[1mRows where Age > 25:\u001b[0m\n",
      "         ID Name  Age  Salary Department\n",
      "emp002  102  Bob   27   60000         IT\n",
      "emp005  105  Eva   30   70000         HR\n",
      "\n",
      "\u001b[1mRows where Age>25 and Department=='IT':\u001b[0m\n",
      "         ID Name  Age  Salary Department\n",
      "emp002  102  Bob   27   60000         IT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Slice first 3 rows using iloc\n",
    "print(f\"{bold}First 3 rows using iloc:{reset}\")\n",
    "print(df.iloc[:3], end=\"\\n\\n\")\n",
    "\n",
    "# Select Salary column using loc\n",
    "print(f\"{bold}Salary column using loc:{reset}\")\n",
    "print(df.loc[:, \"Salary\"], end=\"\\n\\n\")\n",
    "\n",
    "# Conditional selection: Age > 25\n",
    "print(f\"{bold}Rows where Age > 25:{reset}\")\n",
    "print(df[df[\"Age\"] > 25], end=\"\\n\\n\")\n",
    "\n",
    "# Multiple conditions: Age>25 and Department=='IT'\n",
    "print(f\"{bold}Rows where Age>25 and Department=='IT':{reset}\")\n",
    "print(df[(df[\"Age\"] > 25) & (df[\"Department\"] == \"IT\")], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ffd554-4cc6-4262-abfe-5fb05edbb040",
   "metadata": {},
   "source": [
    "# Adding / Modifying Columns\n",
    "\n",
    "We will:\n",
    "- Add a new column 'Bonus' as 10% of Salary  \n",
    "- Modify existing column values  \n",
    "- Drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "818f636b-eb75-4a35-83fe-d04bd759ac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataFrame after adding Bonus column:\u001b[0m\n",
      "         ID     Name  Age  Salary Department   Bonus\n",
      "emp001  101    Alice   23   50000         HR  5000.0\n",
      "emp002  102      Bob   27   60000         IT  6000.0\n",
      "emp003  103  Charlie   22   55000         IT  5500.0\n",
      "emp004  104    David   25   65000    Finance  6500.0\n",
      "emp005  105      Eva   30   70000         HR  7000.0\n",
      "\n",
      "\u001b[1mDataFrame after increasing Salary by 5%:\u001b[0m\n",
      "         ID     Name  Age   Salary Department   Bonus\n",
      "emp001  101    Alice   23  52500.0         HR  5000.0\n",
      "emp002  102      Bob   27  63000.0         IT  6000.0\n",
      "emp003  103  Charlie   22  57750.0         IT  5500.0\n",
      "emp004  104    David   25  68250.0    Finance  6500.0\n",
      "emp005  105      Eva   30  73500.0         HR  7000.0\n",
      "\n",
      "\u001b[1mDataFrame after dropping Bonus column:\u001b[0m\n",
      "         ID     Name  Age   Salary Department\n",
      "emp001  101    Alice   23  52500.0         HR\n",
      "emp002  102      Bob   27  63000.0         IT\n",
      "emp003  103  Charlie   22  57750.0         IT\n",
      "emp004  104    David   25  68250.0    Finance\n",
      "emp005  105      Eva   30  73500.0         HR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new column Bonus\n",
    "df[\"Bonus\"] = df[\"Salary\"] * 0.10\n",
    "\n",
    "print(f\"{bold}DataFrame after adding Bonus column:{reset}\")\n",
    "print(df, end=\"\\n\\n\")\n",
    "\n",
    "# Modify Salary column (increase by 5%)\n",
    "df[\"Salary\"] = df[\"Salary\"] * 1.05\n",
    "print(f\"{bold}DataFrame after increasing Salary by 5%:{reset}\")\n",
    "print(df, end=\"\\n\\n\")\n",
    "\n",
    "# Drop Bonus column\n",
    "df.drop(\"Bonus\", axis=1, inplace=True)\n",
    "print(f\"{bold}DataFrame after dropping Bonus column:{reset}\")\n",
    "print(df, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7f262-c20f-43c7-a098-de124dd0d85c",
   "metadata": {},
   "source": [
    "# Sorting & Ranking\n",
    "\n",
    "We will:\n",
    "- Sort DataFrame by Age and Salary  \n",
    "- Sort by index  \n",
    "- Rank Age values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "676439eb-a1ca-48ce-a6d8-48c6694cebb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataFrame sorted by Age:\u001b[0m\n",
      "         ID     Name  Age   Salary Department\n",
      "emp003  103  Charlie   22  57750.0         IT\n",
      "emp001  101    Alice   23  52500.0         HR\n",
      "emp004  104    David   25  68250.0    Finance\n",
      "emp002  102      Bob   27  63000.0         IT\n",
      "emp005  105      Eva   30  73500.0         HR\n",
      "\n",
      "\u001b[1mDataFrame sorted by Salary descending:\u001b[0m\n",
      "         ID     Name  Age   Salary Department\n",
      "emp005  105      Eva   30  73500.0         HR\n",
      "emp004  104    David   25  68250.0    Finance\n",
      "emp002  102      Bob   27  63000.0         IT\n",
      "emp003  103  Charlie   22  57750.0         IT\n",
      "emp001  101    Alice   23  52500.0         HR\n",
      "\n",
      "\u001b[1mDataFrame sorted by index:\u001b[0m\n",
      "         ID     Name  Age   Salary Department\n",
      "emp001  101    Alice   23  52500.0         HR\n",
      "emp002  102      Bob   27  63000.0         IT\n",
      "emp003  103  Charlie   22  57750.0         IT\n",
      "emp004  104    David   25  68250.0    Finance\n",
      "emp005  105      Eva   30  73500.0         HR\n",
      "\n",
      "\u001b[1mAge ranking:\u001b[0m\n",
      "emp001    2.0\n",
      "emp002    4.0\n",
      "emp003    1.0\n",
      "emp004    3.0\n",
      "emp005    5.0\n",
      "Name: Age, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by Age ascending\n",
    "print(f\"{bold}DataFrame sorted by Age:{reset}\")\n",
    "print(df.sort_values(by=\"Age\"), end=\"\\n\\n\")\n",
    "\n",
    "# Sort by Salary descending\n",
    "print(f\"{bold}DataFrame sorted by Salary descending:{reset}\")\n",
    "print(df.sort_values(by=\"Salary\", ascending=False), end=\"\\n\\n\")\n",
    "\n",
    "# Sort by index\n",
    "print(f\"{bold}DataFrame sorted by index:{reset}\")\n",
    "print(df.sort_index(), end=\"\\n\\n\")\n",
    "\n",
    "# Rank Age\n",
    "print(f\"{bold}Age ranking:{reset}\")\n",
    "print(df[\"Age\"].rank(), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f0a9f2-2a6f-4e97-b3a2-8ce5fc27ecfa",
   "metadata": {},
   "source": [
    "# String & Categorical Operations\n",
    "\n",
    "We will:\n",
    "- Convert 'Department' names to uppercase  \n",
    "- Check which names contain 'a'  \n",
    "- Convert Department column to category type and show codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b284ac6-edd9-4b91-b3c6-b1e2cec670d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDepartment in uppercase:\u001b[0m\n",
      "emp001         HR\n",
      "emp002         IT\n",
      "emp003         IT\n",
      "emp004    FINANCE\n",
      "emp005         HR\n",
      "Name: Department, dtype: object\n",
      "\n",
      "\u001b[1mNames containing 'a':\u001b[0m\n",
      "         ID     Name  Age   Salary Department\n",
      "emp001  101    Alice   23  52500.0         HR\n",
      "emp003  103  Charlie   22  57750.0         IT\n",
      "emp004  104    David   25  68250.0    FINANCE\n",
      "emp005  105      Eva   30  73500.0         HR\n",
      "\n",
      "\u001b[1mDepartment category codes:\u001b[0m\n",
      "emp001    1\n",
      "emp002    2\n",
      "emp003    2\n",
      "emp004    0\n",
      "emp005    1\n",
      "dtype: int8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uppercase Department\n",
    "df[\"Department\"] = df[\"Department\"].str.upper()\n",
    "print(f\"{bold}Department in uppercase:{reset}\")\n",
    "print(df[\"Department\"], end=\"\\n\\n\")\n",
    "\n",
    "# Names containing 'a' (case-insensitive)\n",
    "print(f\"{bold}Names containing 'a':{reset}\")\n",
    "print(df[df[\"Name\"].str.contains(\"a\", case=False)], end=\"\\n\\n\")\n",
    "\n",
    "# Convert to categorical and show codes\n",
    "df[\"Department\"] = df[\"Department\"].astype(\"category\")\n",
    "print(f\"{bold}Department category codes:{reset}\")\n",
    "print(df[\"Department\"].cat.codes, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f435b63-5b07-4c1f-9444-987265344387",
   "metadata": {},
   "source": [
    "# Aggregation & Grouping\n",
    "\n",
    "We will:\n",
    "- Group by Department  \n",
    "- Compute mean Salary and Age per group  \n",
    "- Apply multiple aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ead20972-e09a-4f43-bf00-1c952d0c8488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMean Salary and Age by Department:\u001b[0m\n",
      "             Salary   Age\n",
      "Department               \n",
      "FINANCE     68250.0  25.0\n",
      "HR          63000.0  26.5\n",
      "IT          60375.0  24.5\n",
      "\n",
      "\u001b[1mMultiple aggregations by Department:\u001b[0m\n",
      "             Salary                               Age            \n",
      "               mean       sum      min      max  mean sum min max\n",
      "Department                                                       \n",
      "FINANCE     68250.0   68250.0  68250.0  68250.0  25.0  25  25  25\n",
      "HR          63000.0  126000.0  52500.0  73500.0  26.5  53  23  30\n",
      "IT          60375.0  120750.0  57750.0  63000.0  24.5  49  22  27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by Department and mean (silencing FutureWarning)\n",
    "print(f\"{bold}Mean Salary and Age by Department:{reset}\")\n",
    "print(df.groupby(\"Department\", observed=False)[[\"Salary\", \"Age\"]].mean(), end=\"\\n\\n\")\n",
    "\n",
    "# Multiple aggregation functions\n",
    "print(f\"{bold}Multiple aggregations by Department:{reset}\")\n",
    "print(df.groupby(\"Department\", observed=False)[[\"Salary\", \"Age\"]].agg([\"mean\", \"sum\", \"min\", \"max\"]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66840f58-1f51-4d82-8906-58c2c1e0d617",
   "metadata": {},
   "source": [
    "# Combining / Merging DataFrames\n",
    "\n",
    "We will demonstrate:\n",
    "- Concatenating two DataFrames  \n",
    "- Merging DataFrames on a common column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "009c1ca1-3b2e-48e6-a402-e903df8191fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConcatenated DataFrame:\u001b[0m\n",
      "         ID     Name  Age   Salary Department\n",
      "emp001  101    Alice   23  52500.0         HR\n",
      "emp002  102      Bob   27  63000.0         IT\n",
      "emp003  103  Charlie   22  57750.0         IT\n",
      "emp004  104    David   25  68250.0    FINANCE\n",
      "emp005  105      Eva   30  73500.0         HR\n",
      "emp006  106    Frank   28  72000.0         IT\n",
      "emp007  107    Grace   26  68000.0         HR\n",
      "\n",
      "\u001b[1mMerged DataFrame with Bonus (custom index preserved):\u001b[0m\n",
      "         ID     Name  Age   Salary Department  Bonus\n",
      "emp001  101    Alice   23  52500.0         HR   5000\n",
      "emp002  102      Bob   27  63000.0         IT   6000\n",
      "emp003  103  Charlie   22  57750.0         IT   5500\n",
      "emp004  104    David   25  68250.0    FINANCE   6500\n",
      "emp005  105      Eva   30  73500.0         HR   7000\n",
      "emp006  106    Frank   28  72000.0         IT   7200\n",
      "emp007  107    Grace   26  68000.0         HR   6800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 6️⃣ Combining / Merging DataFrames (Custom index preserved)\n",
    "# -------------------------------\n",
    "\n",
    "# Markdown explanation (for your reference)\n",
    "\"\"\"\n",
    "# Combining / Merging DataFrames\n",
    "\n",
    "We will demonstrate:\n",
    "- Concatenating two DataFrames  \n",
    "- Merging DataFrames on a common column (ID) while keeping the custom index\n",
    "\"\"\"\n",
    "\n",
    "# Create a second small DataFrame\n",
    "data_extra = {\n",
    "    \"ID\": [106, 107],\n",
    "    \"Name\": [\"Frank\", \"Grace\"],\n",
    "    \"Age\": [28, 26],\n",
    "    \"Salary\": [72000, 68000],\n",
    "    \"Department\": [\"IT\", \"HR\"]\n",
    "}\n",
    "df_extra = convert_to_pd(data_extra, indexes=[\"emp006\", \"emp007\"])\n",
    "\n",
    "# Concatenate\n",
    "df_concat = pd.concat([df, df_extra])\n",
    "print(f\"{bold}Concatenated DataFrame:{reset}\")\n",
    "print(df_concat, end=\"\\n\\n\")\n",
    "\n",
    "# Merge example: create a bonus DataFrame\n",
    "bonus_dict = {\n",
    "    \"ID\": [101, 102, 103, 104, 105, 106, 107],\n",
    "    \"Bonus\": [5000, 6000, 5500, 6500, 7000, 7200, 6800]\n",
    "}\n",
    "df_bonus = convert_to_pd(bonus_dict, indexes=[f\"emp{str(i).zfill(3)}\" for i in range(1,8)])\n",
    "\n",
    "# Merge df_concat with df_bonus using index to preserve emp001, emp002...\n",
    "df_merged = df_concat.merge(df_bonus[[\"Bonus\"]], left_index=True, right_index=True)\n",
    "\n",
    "print(f\"{bold}Merged DataFrame with Bonus (custom index preserved):{reset}\")\n",
    "print(df_merged, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae28540-151e-43a0-8b32-9f2c26cf82fc",
   "metadata": {},
   "source": [
    "# Advanced Summary & Info\n",
    "\n",
    "We will:\n",
    "- Use describe() for statistics  \n",
    "- Use info() to check data types, non-null counts, memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "159c6629-1b0b-4b4c-b46b-f2eee05652fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDescriptive statistics of numerical columns:\u001b[0m\n",
      "               ID        Age        Salary\n",
      "count    5.000000   5.000000      5.000000\n",
      "mean   103.000000  25.400000  63000.000000\n",
      "std      1.581139   3.209361   8300.978858\n",
      "min    101.000000  22.000000  52500.000000\n",
      "25%    102.000000  23.000000  57750.000000\n",
      "50%    103.000000  25.000000  63000.000000\n",
      "75%    104.000000  27.000000  68250.000000\n",
      "max    105.000000  30.000000  73500.000000\n",
      "\n",
      "\u001b[1mDataFrame info:\u001b[0m\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5 entries, emp001 to emp005\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   ID          5 non-null      int64   \n",
      " 1   Name        5 non-null      object  \n",
      " 2   Age         5 non-null      int64   \n",
      " 3   Salary      5 non-null      float64 \n",
      " 4   Department  5 non-null      category\n",
      "dtypes: category(1), float64(1), int64(2), object(1)\n",
      "memory usage: 509.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# describe()\n",
    "print(f\"{bold}Descriptive statistics of numerical columns:{reset}\")\n",
    "print(df.describe(), end=\"\\n\\n\")\n",
    "\n",
    "# info()\n",
    "print(f\"{bold}DataFrame info:{reset}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60cc486-2e13-4ecb-b838-8a0da9b18cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
